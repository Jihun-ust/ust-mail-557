{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e415577",
   "metadata": {},
   "source": [
    "\n",
    "# Retrieval‑Augmented Generation (RAG) — Policy Assistant (Gemini 2.5 Flash Lite)\n",
    "\n",
    "**Scenario:** A policy assistant answers employee questions using HR policies, benefits PDFs, and recent announcement emails.  \n",
    "**Goal:** Short, grounded answers with exact citations and a structured JSON for audit — or **abstain** when sources don’t suffice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c26ae0",
   "metadata": {},
   "source": [
    "\n",
    "- Ingesting documents with metadata (source, date, owner, **ACL**)\n",
    "- Chunking & indexing; **BM25**, simple **semantic** cosine; **Hybrid** + **MMR** diversity\n",
    "- Building a strict, source-bound prompt that **requires citations** and allows abstention\n",
    "- Calling **Gemini 2.5 Flash Lite** for low-temperature JSON outputs\n",
    "- Validating a JSON **contract** (with `None` fallbacks) and computing offline **RAG metrics**\n",
    "- Logging retrieval hits and feeding failures back to improve data/prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a742e",
   "metadata": {},
   "source": [
    "## Setup — SDK & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
    "    _GEMINI_READY = bool(userdata.get('GOOGLE_API_KEY'))\n",
    "except Exception:\n",
    "    print(\"Install google-generativeai to enable live calls.\")\n",
    "    _GEMINI_READY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, time, json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "MODEL_NAME = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash-lite\")\n",
    "\n",
    "if not _GEMINI_READY:\n",
    "    print(\"WARNING: GOOGLE_API_KEY is not set. Set it before running LLM cells.\")\n",
    "else:\n",
    "    print(\"GOOGLE_API_KEY detected. Model:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa35f6",
   "metadata": {},
   "source": [
    "\n",
    "## Ingest — Mock Corpus with Metadata & ACL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    {\"doc_id\":\"HR-Handbook-2025.pdf\",\"source_type\":\"pdf\",\"date\":\"2025-06-15\",\"owner\":\"HR\",\n",
    "     \"acl\":{\"departments\":[\"all\"],\"locations\":[\"all\"]},\n",
    "     \"pages\":{\"11\":\"Benefits overview. Health, dental, vision. Parental leave policy updated in 2025.\",\n",
    "              \"12\":\"Parental leave: 16 weeks paid for primary caregivers; 8 weeks paid for secondary caregivers. Eligibility: 6 months tenure.\",\n",
    "              \"28\":\"Sabbatical program: 8 weeks unpaid after 4 years of service; manager approval required.\"}},\n",
    "    {\"doc_id\":\"Benefits-Site\",\"url\":\"https://intranet/hr/benefits#leave\",\"source_type\":\"web\",\"date\":\"2025-07-20\",\"owner\":\"HR\",\n",
    "     \"acl\":{\"departments\":[\"all\"],\"locations\":[\"US\",\"CA\",\"UK\"]},\n",
    "     \"sections\":{\"leave\":\"Paid time off, sick leave, parental leave details; links to eligibility tables and forms.\",\n",
    "                 \"health\":\"Medical plans with HSA and FSA options; contact benefits@company.example\"}},\n",
    "    {\"doc_id\":\"Announcement-Email-2025-08-05\",\"source_type\":\"email\",\"date\":\"2025-08-05\",\"owner\":\"HR\",\n",
    "     \"acl\":{\"departments\":[\"all\"],\"locations\":[\"all\"]},\n",
    "     \"body\":\"Reminder: parental leave remains 16 weeks for primary caregivers; remote employees in UK now receive an additional 2 weeks statutory-compatible leave.\"},\n",
    "    {\"doc_id\":\"Policy-Wiki-Travel\",\"source_type\":\"wiki\",\"date\":\"2025-05-10\",\"owner\":\"Finance\",\n",
    "     \"acl\":{\"departments\":[\"Engineering\",\"Finance\"],\"locations\":[\"US\",\"CA\"]},\n",
    "     \"sections\":{\"per_diem\":\"US/CA per diem allowances by city tier. Submit receipts via Concur.\",\n",
    "                 \"intl_travel\":\"Visa and insurance checklist. Book via corporate portal.\"}},\n",
    "    {\"doc_id\":\"Ticket-History-1234\",\"source_type\":\"ticket\",\"date\":\"2025-03-11\",\"owner\":\"HR\",\n",
    "     \"acl\":{\"departments\":[\"all\"],\"locations\":[\"all\"]},\n",
    "     \"body\":\"Resolved: clarification on PTO accrual rates. Linked to HR-Handbook-2025.pdf p.10.\"}\n",
    "]\n",
    "\n",
    "def flatten_passages(corpus):\n",
    "    passages = []\n",
    "    for d in corpus:\n",
    "        meta = {k:d.get(k) for k in [\"doc_id\",\"source_type\",\"date\",\"owner\",\"acl\"]}\n",
    "        if d.get(\"pages\"):\n",
    "            for p, text in d[\"pages\"].items():\n",
    "                passages.append({**meta, \"section_or_page\": str(p), \"text\": text, \"url\": None})\n",
    "        if d.get(\"sections\"):\n",
    "            for s, text in d[\"sections\"].items():\n",
    "                passages.append({**meta, \"section_or_page\": str(s), \"text\": text, \"url\": d.get(\"url\")})\n",
    "        if d.get(\"body\"):\n",
    "            passages.append({**meta, \"section_or_page\": \"body\", \"text\": d[\"body\"], \"url\": d.get(\"url\")})\n",
    "    return passages\n",
    "\n",
    "PASSAGES = flatten_passages(CORPUS)\n",
    "len(PASSAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0f433",
   "metadata": {},
   "source": [
    "\n",
    "## Index — Tokenize, BM25, TF‑IDF (Simple Semantic), and Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD = re.compile(r\"[A-Za-z0-9_#@]+\")\n",
    "def tokenize(text): return [w.lower() for w in WORD.findall(text or \"\")]\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "DOC_TOKENS = [tokenize(p[\"text\"]) for p in PASSAGES]\n",
    "N_DOCS = len(DOC_TOKENS)\n",
    "\n",
    "df = Counter()\n",
    "for toks in DOC_TOKENS:\n",
    "    for t in set(toks):\n",
    "        df[t] += 1\n",
    "idf = {t: math.log((N_DOCS - c + 0.5)/(c + 0.5) + 1) for t, c in df.items()}\n",
    "\n",
    "avgdl = sum(len(toks) for toks in DOC_TOKENS)/N_DOCS\n",
    "k1, b = 1.5, 0.75\n",
    "\n",
    "def bm25(query, k=50):\n",
    "    q = tokenize(query)\n",
    "    scores = []\n",
    "    for i, toks in enumerate(DOC_TOKENS):\n",
    "        dl = len(toks); tf = Counter(toks); s = 0.0\n",
    "        for t in q:\n",
    "            if t in tf and t in idf:\n",
    "                num = tf[t]*(k1+1); den = tf[t] + k1*(1 - b + b*dl/avgdl)\n",
    "                s += idf[t]*(num/den)\n",
    "        scores.append((s,i))\n",
    "    scores.sort(reverse=True)\n",
    "    return scores[:k]\n",
    "\n",
    "def tfidf_vec(tokens):\n",
    "    tf = Counter(tokens)\n",
    "    return {t:(1+math.log(1+c))*idf[t] for t,c in tf.items() if t in idf}\n",
    "\n",
    "def cosine(v1, v2):\n",
    "    if not v1 or not v2: return 0.0\n",
    "    num = sum(v1.get(t,0)*v2.get(t,0) for t in set(v1)|set(v2))\n",
    "    den = math.sqrt(sum(x*x for x in v1.values())) * math.sqrt(sum(x*x for x in v2.values()))\n",
    "    return num/den if den else 0.0\n",
    "\n",
    "DOC_TFIDF = [tfidf_vec(toks) for toks in DOC_TOKENS]\n",
    "\n",
    "def semantic_scores(query, k=50):\n",
    "    qv = tfidf_vec(tokenize(query))\n",
    "    scores = [(cosine(qv, dv), i) for i, dv in enumerate(DOC_TFIDF)]\n",
    "    scores.sort(reverse=True)\n",
    "    return scores[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae321c01",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieve — Hybrid → ACL Filter → Rerank → MMR Diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_scores(pairs):\n",
    "    if not pairs: return []\n",
    "    smax = max(s for s,_ in pairs) or 1.0\n",
    "    return [(s/smax, i) for s,i in pairs]\n",
    "\n",
    "def hybrid_retrieve(query, user_department=\"all\", user_location=\"US\", k_cand=50, top_k=8, lambda_sem=0.5):\n",
    "    bm = normalize_scores(bm25(query, k=k_cand))\n",
    "    se = normalize_scores(semantic_scores(query, k=k_cand))\n",
    "    combined = defaultdict(float)\n",
    "    for s,i in bm: combined[i] += (1 - lambda_sem)*s\n",
    "    for s,i in se: combined[i] += lambda_sem*s\n",
    "\n",
    "    def acl_ok(p):\n",
    "        acl = p[\"acl\"]\n",
    "        dep_ok = \"all\" in acl[\"departments\"] or user_department in acl[\"departments\"]\n",
    "        loc_ok = \"all\" in acl[\"locations\"] or user_location in acl[\"locations\"]\n",
    "        return dep_ok and loc_ok\n",
    "\n",
    "    items = [(sc, i) for i, sc in combined.items() if acl_ok(PASSAGES[i])]\n",
    "    items.sort(reverse=True)\n",
    "\n",
    "    def recency_weight(p):\n",
    "        try: return 1.0 + 0.01*int(p[\"date\"].replace(\"-\",\"\"))\n",
    "        except: return 1.0\n",
    "    items = [(s*recency_weight(PASSAGES[i]), i) for s,i in items]\n",
    "    items.sort(reverse=True)\n",
    "\n",
    "    selected, selected_vecs = [], []\n",
    "    qv = tfidf_vec(tokenize(query))\n",
    "    cand = [i for _,i in items[:k_cand]]\n",
    "    while cand and len(selected) < top_k:\n",
    "        scores = []\n",
    "        for i in cand:\n",
    "            sim_q = cosine(qv, DOC_TFIDF[i])\n",
    "            sim_sel = max([cosine(DOC_TFIDF[i], v) for v in selected_vecs] + [0.0])\n",
    "            mmr = 0.7*sim_q - 0.3*sim_sel\n",
    "            scores.append((mmr, i))\n",
    "        scores.sort(reverse=True)\n",
    "        best = scores[0][1]\n",
    "        selected.append(best)\n",
    "        selected_vecs.append(DOC_TFIDF[best])\n",
    "        cand.remove(best)\n",
    "    return selected\n",
    "\n",
    "def render_sources(indices):\n",
    "    lines = []\n",
    "    for r, i in enumerate(indices, 1):\n",
    "        p = PASSAGES[i]\n",
    "        src = f\"{p['doc_id']}:{p['section_or_page']}\"\n",
    "        lines.append(f\"{r}) {src} — {p['text']}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b271f",
   "metadata": {},
   "source": [
    "\n",
    "## Contract — Answer JSON (with `None` fallbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_SCHEMA = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"answer\": {\"type\":\"string\"},\n",
    "    \"citations\": {\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\n",
    "        \"doc_id\":{\"type\":\"string\"}, \"page\":{\"type\":\"string\"}, \"section\":{\"type\":\"string\"}, \"url\":{\"type\":\"string\"}\n",
    "    }}},\n",
    "    \"confidence\": {\"type\":\"string\",\"enum\":[\"low\",\"medium\",\"high\"]},\n",
    "    \"rationale_internal\": {\"type\":\"string\"}\n",
    "  },\n",
    "  \"required\": [\"answer\",\"citations\",\"confidence\"],\n",
    "  \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "def validate_with_fallbacks(obj, schema=ANSWER_SCHEMA):\n",
    "    errors = []\n",
    "    props = schema.get(\"properties\", {})\n",
    "    required = schema.get(\"required\", [])\n",
    "    fallbacks = {\"string\":\"\", \"array\":[]}\n",
    "    norm = {}\n",
    "    for k,v in obj.items():\n",
    "        spec = props.get(k)\n",
    "        if spec and v is None:\n",
    "            if spec.get(\"type\") == \"string\": norm[k] = fallbacks[\"string\"]\n",
    "            elif spec.get(\"type\") == \"array\": norm[k] = fallbacks[\"array\"]\n",
    "            else: norm[k] = v\n",
    "        else:\n",
    "            norm[k] = v\n",
    "    for key in required:\n",
    "        if key not in norm: errors.append(f\"Missing required field: {key}\")\n",
    "    if schema.get(\"additionalProperties\") is False:\n",
    "        for k in norm.keys():\n",
    "            if k not in props: errors.append(f\"Unexpected field: {k}\")\n",
    "    for k,spec in props.items():\n",
    "        if k not in norm: continue\n",
    "        val = norm[k]\n",
    "        if spec.get(\"type\") == \"string\":\n",
    "            if not isinstance(val, str): errors.append(f\"Field {k} must be string\")\n",
    "            if \"enum\" in spec and val and val not in spec[\"enum\"]:\n",
    "                errors.append(f\"Field {k} not in enum {spec['enum']} (got: {val})\")\n",
    "        elif spec.get(\"type\") == \"array\":\n",
    "            if not isinstance(val, list): errors.append(f\"Field {k} must be array\")\n",
    "    return errors, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af60fc",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt Construction — Strict, Source‑Bound, with Abstention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc189b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN_TEXT = \"Insufficient information in approved sources.\"\n",
    "\n",
    "def build_prompt(question, sources_text, schema):\n",
    "    contract = json.dumps(schema, ensure_ascii=False, indent=2)\n",
    "    return f\"\"\"SYSTEM: You answer using only the provided sources. Cite each claim. If the answer isn’t present, reply exactly: \"{ABSTAIN_TEXT}\".\n",
    "\n",
    "USER: {question}\n",
    "\n",
    "SOURCES:\n",
    "{sources_text}\n",
    "\n",
    "RESPONSE FORMAT (JSON only):\n",
    "{contract}\n",
    "\n",
    "Rules:\n",
    "- Use only claims that appear in SOURCES.\n",
    "- Cite each claim with doc_id and page or section if available.\n",
    "- If insufficient info, set \"answer\" to \"{ABSTAIN_TEXT}\" and return empty citations.\n",
    "- Keep \"rationale_internal\" brief.\n",
    "- Tone: concise, neutral, actionable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b9a25",
   "metadata": {},
   "source": [
    "## Generate — Call Gemini 2.5 Flash Lite (JSON only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bcf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_json(prompt):\n",
    "    if not _GEMINI_READY:\n",
    "        raise RuntimeError(\"GOOGLE_API_KEY not configured or SDK not available.\")\n",
    "    model = genai.GenerativeModel(MODEL_NAME)\n",
    "    t0 = time.time()\n",
    "    resp = model.generate_content(prompt)\n",
    "    latency = time.time() - t0\n",
    "    text = getattr(resp, \"text\", None)\n",
    "    if not text and hasattr(resp, \"candidates\") and resp.candidates:\n",
    "        parts = getattr(resp.candidates[0].content, \"parts\", None)\n",
    "        if parts and hasattr(parts[0], \"text\"):\n",
    "            text = parts[0].text\n",
    "    if not text: raise RuntimeError(\"Empty response from model\")\n",
    "    s = text.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        s = s.strip(\"`\"); s = s.split(\"\\n\",1)[1] if \"\\n\" in s else s\n",
    "    start = s.find(\"{\"); end = s.rfind(\"}\")\n",
    "    if start >= 0 and end > start: s = s[start:end+1]\n",
    "    obj = json.loads(s)\n",
    "    return obj, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2261bb",
   "metadata": {},
   "source": [
    "\n",
    "## Orchestrator — Retrieve → Prompt → Generate → Validate → Post‑process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIT = []\n",
    "\n",
    "def within_acl(p, dept, loc):\n",
    "    acl = p[\"acl\"]\n",
    "    return (\"all\" in acl[\"departments\"] or dept in acl[\"departments\"]) and (\"all\" in acl[\"locations\"] or loc in acl[\"locations\"])\n",
    "\n",
    "def generate_answer(question, user_department=\"all\", user_location=\"US\", top_k=6):\n",
    "    idxs = hybrid_retrieve(question, user_department=user_department, user_location=user_location, top_k=top_k)\n",
    "    idxs = [i for i in idxs if within_acl(PASSAGES[i], user_department, user_location)]\n",
    "    sources_text = render_sources(idxs)\n",
    "    prompt = build_prompt(question, sources_text, ANSWER_SCHEMA)\n",
    "    raw, latency = call_gemini_json(prompt)\n",
    "    errors, norm = validate_with_fallbacks(raw)\n",
    "\n",
    "    if (norm.get(\"answer\") or \"\").strip() == ABSTAIN_TEXT:\n",
    "        norm[\"citations\"] = []\n",
    "        norm[\"confidence\"] = \"low\"\n",
    "    if not norm.get(\"citations\"):\n",
    "        norm[\"citations\"] = []\n",
    "        for i in idxs[:2]:\n",
    "            p = PASSAGES[i]\n",
    "            norm[\"citations\"].append({\n",
    "                \"doc_id\": p[\"doc_id\"],\n",
    "                \"page\": p[\"section_or_page\"] if p[\"source_type\"]==\"pdf\" else \"\",\n",
    "                \"section\": p[\"section_or_page\"] if p[\"source_type\"]!=\"pdf\" else \"\",\n",
    "                \"url\": p.get(\"url\") or \"\"\n",
    "            })\n",
    "\n",
    "    errors2, norm2 = validate_with_fallbacks(norm)\n",
    "    AUDIT.append({\n",
    "        \"ts\": datetime.utcnow().isoformat()+\"Z\",\n",
    "        \"q\": question, \"dept\": user_department, \"loc\": user_location,\n",
    "        \"retrieved\": [f\"{PASSAGES[i]['doc_id']}:{PASSAGES[i]['section_or_page']}\" for i in idxs],\n",
    "        \"latency_ms\": int(latency*1000), \"errors\": errors+errors2\n",
    "    })\n",
    "    return norm2, idxs, latency, errors+errors2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718785c5",
   "metadata": {},
   "source": [
    "\n",
    "## Offline Evaluation — Golden Set & RAG Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ab902",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_SET = [\n",
    "    {\"id\":\"e1\",\"q\":\"How many weeks of paid parental leave do primary caregivers get?\",\"dept\":\"all\",\"loc\":\"US\",\n",
    "     \"expect\":{\"must_contain\":[\"16 weeks\"],\"should_cite\":[\"HR-Handbook-2025.pdf\",\"Benefits-Site\"]}},\n",
    "    {\"id\":\"e2\",\"q\":\"Do UK remote employees get extra parental leave?\",\"dept\":\"all\",\"loc\":\"UK\",\n",
    "     \"expect\":{\"must_contain\":[\"additional 2 weeks\"],\"should_cite\":[\"Announcement-Email-2025-08-05\"]}},\n",
    "    {\"id\":\"e3\",\"q\":\"What are the per diem rules for Canada engineering travel?\",\"dept\":\"Engineering\",\"loc\":\"CA\",\n",
    "     \"expect\":{\"must_contain\":[\"per diem\"],\"should_cite\":[\"Policy-Wiki-Travel\"]}},\n",
    "    {\"id\":\"e4\",\"q\":\"What is the 401(k) employer match policy?\",\"dept\":\"all\",\"loc\":\"US\",\n",
    "     \"expect\":{\"abstain\": True}}\n",
    "]\n",
    "\n",
    "def grounded(pred, idxs):\n",
    "    cited_ids = set([c.get(\"doc_id\") for c in (pred.get(\"citations\") or []) if isinstance(c, dict)])\n",
    "    retrieved_ids = set([PASSAGES[i][\"doc_id\"] for i in idxs])\n",
    "    if (pred.get(\"answer\") or \"\") == ABSTAIN_TEXT: return True\n",
    "    return bool(cited_ids) and cited_ids.issubset(retrieved_ids)\n",
    "\n",
    "def contains_required_text(pred, must_list):\n",
    "    ans = (pred.get(\"answer\") or \"\").lower()\n",
    "    return all(m.lower() in ans for m in must_list)\n",
    "\n",
    "def abstention_correct(pred, expect):\n",
    "    wants = expect.get(\"abstain\", False)\n",
    "    is_abstain = (pred.get(\"answer\") or \"\") == ABSTAIN_TEXT\n",
    "    return wants == is_abstain\n",
    "\n",
    "def evaluate(dataset):\n",
    "    rows, latencies = [], []\n",
    "    for ex in dataset:\n",
    "        pred, idxs, latency, errs = generate_answer(ex[\"q\"], ex[\"dept\"], ex[\"loc\"])\n",
    "        lat_ms = round(latency*1000.0,1); latencies.append(lat_ms)\n",
    "        rows.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"latency_ms\": lat_ms,\n",
    "            \"grounded\": grounded(pred, idxs),\n",
    "            \"abstention_ok\": abstention_correct(pred, ex[\"expect\"]),\n",
    "            \"must_contain_ok\": contains_required_text(pred, ex[\"expect\"].get(\"must_contain\", [])),\n",
    "            \"pred\": pred,\n",
    "            \"retrieved\": [f\"{PASSAGES[i]['doc_id']}:{PASSAGES[i]['section_or_page']}\" for i in idxs]\n",
    "        })\n",
    "    agg = {\n",
    "        \"grounded_rate\": sum(1 for r in rows if r[\"grounded\"])/len(rows),\n",
    "        \"abstention_correct_rate\": sum(1 for r in rows if r[\"abstention_ok\"])/len(rows),\n",
    "        \"must_contain_rate\": sum(1 for r in rows if r[\"must_contain_ok\"])/len(rows),\n",
    "        \"latency_p95_ms\": sorted(latencies)[int(0.95*len(latencies))-1] if len(latencies)>1 else latencies[0]\n",
    "    }\n",
    "    return rows, agg\n",
    "\n",
    "if _GEMINI_READY:\n",
    "    preview_pred, preview_idxs, preview_latency, preview_errs = generate_answer(\"How long is parental leave for primary caregivers?\", \"all\", \"US\")\n",
    "    print(\"Preview citations:\", preview_pred.get(\"citations\"))\n",
    "    print(\"Preview answer:\", preview_pred.get(\"answer\")[:120], \"...\")\n",
    "else:\n",
    "    print(\"Set GOOGLE_API_KEY to run generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf9262",
   "metadata": {},
   "source": [
    "## Run Offline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88deb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _GEMINI_READY:\n",
    "    rows, agg = evaluate(EVAL_SET)\n",
    "    import json\n",
    "    print(\"Aggregate metrics:\")\n",
    "    print(json.dumps(agg, indent=2))\n",
    "    print(\"\\nFirst result:\")\n",
    "    print(json.dumps(rows[0], indent=2))\n",
    "else:\n",
    "    print(\"Set GOOGLE_API_KEY to evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65319c8",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval Strategy Cheatsheet\n",
    "- **Semantic (vectors)** → open‑ended questions on long text; rerank for precision  \n",
    "- **Keyword (BM25)** → short queries, exact terms/codes  \n",
    "- **Hybrid** → robust default (BM25 + semantic)  \n",
    "- **Reranking (cross‑encoder)** → use on small candidate set (e.g., top‑50)  \n",
    "- **MMR/diversity** → avoid duplicates; improve coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738d631",
   "metadata": {},
   "source": [
    "\n",
    "## Next Steps\n",
    "- Replace TF‑IDF with true **embeddings**; add cross‑encoder reranker  \n",
    "- Enrich corpus with **fresh** announcements; prefer recency in reranks  \n",
    "- Add **JSON mode** or function calling; stricter citation validators  \n",
    "- Hook into real **ACL**/identity and doc stores; log misses to refine chunks/prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec820c1",
   "metadata": {},
   "source": [
    "## Audit Log (last 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in AUDIT[-10:]:\n",
    "    import json\n",
    "    print(json.dumps(row, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
