{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a37687",
   "metadata": {},
   "source": [
    "\n",
    "# Reinforcement Learning Example – Q-Learning in Grid World\n",
    "\n",
    "This notebook demonstrates a basic example of **Reinforcement Learning** using **Q-learning** in a simple grid world.\n",
    "\n",
    "- **Agent** interacts with an **environment**\n",
    "- Learns through **trial and error** using **rewards**\n",
    "- Learns an optimal **policy** for navigating the environment\n",
    "\n",
    "We’ll simulate a **5x5 grid** where the goal is to reach a reward cell while avoiding penalty cells.\n",
    "\n",
    "**NOTE: Reinforcement learning does not always succeed. When it fails, reset the environment and try again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8162c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid environment\n",
    "grid_size = 5\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_map = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1)    # right\n",
    "}\n",
    "\n",
    "# Rewards\n",
    "goal = (4, 4)\n",
    "trap = (2, 3)\n",
    "reward_map = np.zeros((grid_size, grid_size))\n",
    "reward_map[goal] = 1\n",
    "reward_map[trap] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1       # learning rate\n",
    "gamma = 0.9       # discount factor\n",
    "epsilon = 0.2     # exploration rate\n",
    "episodes = 700\n",
    "rewards = []\n",
    "\n",
    "# Training\n",
    "for ep in range(episodes):\n",
    "    state = (0, 0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(len(actions))  # explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        move = action_map[action]\n",
    "        next_state = (max(0, min(grid_size - 1, state[0] + move[0])),\n",
    "                      max(0, min(grid_size - 1, state[1] + move[1])))\n",
    "\n",
    "        reward = reward_map[next_state]\n",
    "        best_next = np.max(q_table[next_state])\n",
    "        q_table[state + (action,)] += alpha * (reward + gamma * best_next - q_table[state + (action,)])\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        if next_state == goal or next_state == trap:\n",
    "            done = True\n",
    "\n",
    "        state = next_state\n",
    "    rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df880254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot moving average rewards\n",
    "def moving_avg(data, window=100):\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(moving_avg(rewards), label='Reward')\n",
    "plt.axhline(y=0.78, color='r', linestyle='--', label='Solved Threshold (78%)')\n",
    "plt.title(\"Reward Progression\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Success Rate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy map\n",
    "policy = np.full((grid_size, grid_size), '', dtype=object)\n",
    "arrow_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i, j) == goal:\n",
    "            policy[i, j] = 'Goal'\n",
    "        elif (i, j) == trap:\n",
    "            policy[i, j] = 'X'\n",
    "        else:\n",
    "            best_action = np.argmax(q_table[i, j])\n",
    "            policy[i, j] = arrow_map[best_action]\n",
    "\n",
    "# Display policy\n",
    "print(\"Optimal Policy:\")\n",
    "for row in policy:\n",
    "    print(' '.join(row))\n",
    "print(\"\")\n",
    "print(\"Start: top-left corner\")\n",
    "print(\"Trap: X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of max Q-values\n",
    "q_max = np.max(q_table, axis=2)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(q_max, annot=policy, fmt='', cmap=\"YlGnBu\", cbar_kws={'label': 'Max Q-Value'})\n",
    "plt.title(\"Learned Policy (Q-Learning)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ccc8d",
   "metadata": {},
   "source": [
    "\n",
    "## What Did We Learn?\n",
    "\n",
    "- The agent learned the **optimal path** to the goal while avoiding the trap.\n",
    "- Q-values represent the **expected long-term reward** from each state-action pair.\n",
    "- The policy uses arrows (↑ ↓ ← →) to show the best action at each cell.\n",
    "\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Concept      | Meaning |\n",
    "|--------------|---------|\n",
    "| **State**    | Current position of the agent |\n",
    "| **Action**   | Move taken by the agent |\n",
    "| **Reward**   | Feedback from environment |\n",
    "| **Policy**   | Best action to take in each state |\n",
    "| **Q-Value**  | Expected reward for a state-action |\n",
    "\n",
    "> Reinforcement learning is useful in robotics, game AI, operations research, and adaptive control systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
