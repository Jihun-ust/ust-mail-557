{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6500fec4",
   "metadata": {},
   "source": [
    "# Density-Based Clustering — DBSCAN / HDBSCAN\n",
    "\n",
    "Use a k-distance plot to pick eps, run DBSCAN to discover shape-based clusters and noise, and optionally compare to HDBSCAN (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76cfece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Unsupervised/unsup_utils.py\n",
    "import unsup_utils as utils\n",
    "csv_path = \"https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Unsupervised/unsup.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "X, cols, sc = utils.feature_matrix(df, use_emb=True)\n",
    "\n",
    "kd = utils.k_distance_plot(X, k=5)\n",
    "plt.figure(figsize=(8,3.5)); plt.plot(kd); plt.title(\"k-distance plot (k=5)\"); plt.xlabel(\"points (sorted)\"); plt.ylabel(\"distance to 5th neighbor\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Heuristic eps choice: knee around 80th percentile\n",
    "eps = float(np.percentile(kd, 80))\n",
    "db = DBSCAN(eps=eps, min_samples=10, n_jobs=-1).fit(X)\n",
    "df[\"cluster_db\"] = db.labels_  # -1 = noise\n",
    "\n",
    "X2, _ = utils.pca_2d(X)\n",
    "utils.plot_xy(X2, title=\"PCA (colored by DBSCAN label)\", labels=df[\"cluster_db\"].values)\n",
    "\n",
    "# Try HDBSCAN if installed\n",
    "try:\n",
    "    import hdbscan\n",
    "    hdb = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=10).fit(X)\n",
    "    df[\"cluster_hdb\"] = hdb.labels_\n",
    "    utils.plot_xy(X2, title=\"PCA (colored by HDBSCAN label)\", labels=df[\"cluster_hdb\"].values)\n",
    "except Exception as e:\n",
    "    print(\"hdbscan not installed; skipping HDBSCAN demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3504e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN: highlight the chosen eps on the existing k-distance curve\n",
    "plt.figure(figsize=(8,3.5))\n",
    "plt.plot(kd)\n",
    "plt.axhline(y=eps, linestyle=\"--\")\n",
    "plt.title(f\"k-distance plot (k=5) — chosen eps ≈ {eps:.3f}\")\n",
    "plt.xlabel(\"points (sorted)\"); plt.ylabel(\"distance to 5th neighbor\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# HDBSCAN: condensed tree with selection (if available)\n",
    "try:\n",
    "    import hdbscan\n",
    "    if 'hdb' in globals() and hasattr(hdb, \"condensed_tree_\"):\n",
    "        _ = hdb.condensed_tree_.plot(select_clusters=True, label_clusters=True)\n",
    "        plt.title(f\"HDBSCAN condensed tree (min_cluster_size={getattr(hdb, 'min_cluster_size', 'n/a')}, \"\n",
    "                  f\"min_samples={getattr(hdb, 'min_samples', 'n/a')})\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "    else:\n",
    "        print(\"HDBSCAN object not present or no condensed_tree_; skipping condensed tree plot.\")\n",
    "except Exception as e:\n",
    "    print(\"hdbscan not installed or plotting failed; skipping condensed tree.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster summary with sizes & plain-language names\n",
    "# Work with DBSCAN first; fall back to HDBSCAN if DBSCAN labels absent\n",
    "label_col = \"cluster_db\" if \"cluster_db\" in df.columns else (\"cluster_hdb\" if \"cluster_hdb\" in df.columns else None)\n",
    "if label_col is None:\n",
    "    raise RuntimeError(\"No cluster label column found (expected cluster_db or cluster_hdb).\")\n",
    "\n",
    "# Sizes (include noise = -1)\n",
    "sizes = df[label_col].value_counts().sort_index()\n",
    "display(pd.DataFrame({\"label\": sizes.index, \"size\": sizes.values}))\n",
    "\n",
    "# Numeric feature set excluding embeddings (emb*) and non-features\n",
    "non_feat = {label_col, \"ts\", \"id\"}\n",
    "num_cols = [c for c in X.columns.tolist() if c in getattr(df, 'columns', [])] if hasattr(X, \"columns\") else []\n",
    "# If X is ndarray, infer from original 'cols'\n",
    "if not num_cols and 'cols' in globals():\n",
    "    num_cols = list(cols)\n",
    "\n",
    "num_cols = [c for c in num_cols\n",
    "            if c not in non_feat\n",
    "            and (not str(c).startswith(\"emb\"))\n",
    "            and np.issubdtype(df[c].dtype, np.number)]\n",
    "\n",
    "if len(num_cols) == 0:\n",
    "    print(\"No numeric columns (excluding emb*) available for plain-language summaries.\")\n",
    "    summaries = pd.DataFrame({\"label\": sizes.index, \"size\": sizes.values, \"summary\": \"n/a\"})\n",
    "else:\n",
    "    scaler = StandardScaler().fit(df[num_cols])\n",
    "    Znum = pd.DataFrame(scaler.transform(df[num_cols]), columns=num_cols, index=df.index)\n",
    "\n",
    "    rows = []\n",
    "    for k in sorted(df[label_col].unique()):\n",
    "        idx = df[label_col] == k\n",
    "        if idx.sum() < 2:\n",
    "            summary = \"too few points\"\n",
    "        else:\n",
    "            mean_z = Znum[idx].mean().sort_values(ascending=False)\n",
    "            top_pos = mean_z.head(3).index.tolist()\n",
    "            top_neg = mean_z.tail(3).index.tolist()\n",
    "            summary = f\"↑ {', '.join(top_pos)} | ↓ {', '.join(top_neg)}\"\n",
    "        rows.append({\"label\": int(k), \"size\": int(idx.sum()), \"summary\": summary})\n",
    "    summaries = pd.DataFrame(rows).sort_values(\"label\").reset_index(drop=True)\n",
    "\n",
    "# Auto names & simple playbooks\n",
    "def make_name(summary_text: str) -> str:\n",
    "    ups = [t.strip() for t in summary_text.split(\"|\")[0].replace(\"↑\",\"\").split(\",\")]\n",
    "    ups = [u for u in ups if u][:2]\n",
    "    return \" & \".join(ups) if ups else (\"Noise\" if \"−1\" in str(summary_text) else \"Cluster\")\n",
    "\n",
    "def make_playbook(summary_text: str) -> str:\n",
    "    ups = [t.strip() for t in summary_text.split(\"|\")[0].replace(\"↑\",\"\").split(\",\") if t.strip()]\n",
    "    downs = [t.strip() for t in summary_text.split(\"|\")[-1].replace(\"↓\",\"\").split(\",\") if t.strip()]\n",
    "    return (f\"Lean into {', '.join(ups[:3])}; watch {', '.join(downs[:2])}. \"\n",
    "            \"Pilot targeted actions and track next-period lift.\")\n",
    "\n",
    "summaries[\"name\"] = summaries[\"summary\"].apply(make_name)\n",
    "summaries[\"playbook\"] = summaries[\"summary\"].apply(make_playbook)\n",
    "display(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette on non-noise + stability via bootstraps\n",
    "labels = df[label_col].values\n",
    "mask_nn = labels != -1\n",
    "if mask_nn.sum() > 1 and len(np.unique(labels[mask_nn])) > 1:\n",
    "    try:\n",
    "        sil_nn = silhouette_score(X[mask_nn] if isinstance(X, np.ndarray) else X.values[mask_nn], labels[mask_nn])\n",
    "        print(f\"Silhouette (non-noise): {sil_nn:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Silhouette failed:\", e)\n",
    "else:\n",
    "    print(\"Not enough non-noise clusters for silhouette.\")\n",
    "\n",
    "# Bootstrap stability: fit DBSCAN on bootstrap samples and ARI vs original on overlap\n",
    "def dbscan_bootstrap_ari(Xarr, base_labels, eps, min_samples=10, n_boot=8, frac=0.8, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    aris = []\n",
    "    n = Xarr.shape[0]\n",
    "    for b in range(n_boot):\n",
    "        idx = rng.choice(n, int(n*frac), replace=True)\n",
    "        db_b = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1).fit(Xarr[idx])\n",
    "        # Compare labels on sampled indices\n",
    "        aris.append(adjusted_rand_score(base_labels[idx], db_b.labels_))\n",
    "    return np.array(aris)\n",
    "\n",
    "if label_col == \"cluster_db\":\n",
    "    Xarr = X if isinstance(X, np.ndarray) else X.values\n",
    "    aris = dbscan_bootstrap_ari(Xarr, labels, eps=eps, min_samples=10, n_boot=10)\n",
    "    print(f\"Bootstrap ARI (DBSCAN): mean={aris.mean():.3f} ± {aris.std():.3f}\")\n",
    "else:\n",
    "    print(\"Bootstrap stability for HDBSCAN not implemented here (requires prediction utilities).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf63252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise triage plan example (owners & SLA)\n",
    "noise_mask = (df[label_col] == -1)\n",
    "noise_rate = noise_mask.mean()\n",
    "print(f\"Noise label rate: {noise_rate:.3%}  (count = {noise_mask.sum()})\")\n",
    "\n",
    "triage = pd.DataFrame([\n",
    "    {\"priority\":\"P1 (weekly)\",  \"owner\":\"Data Eng\",     \"action\":\"Check feature scaling/missingness; recompute KD plot; verify eps/min_samples.\", \"SLA\":\"7 days\"},\n",
    "    {\"priority\":\"P1 (weekly)\",  \"owner\":\"Analytics\",    \"action\":\"Sample 50 noise points; inspect PCA distance; label drift check.\",            \"SLA\":\"7 days\"},\n",
    "    {\"priority\":\"P2 (monthly)\",\"owner\":\"Marketing Ops\",\"action\":\"If segment-skewed noise >10pp, adjust routing thresholds or add rules.\",       \"SLA\":\"30 days\"},\n",
    "    {\"priority\":\"P3 (quarter)\",\"owner\":\"Product\",       \"action\":\"Consider richer features or alt. algorithm (HDBSCAN/OPTICS).\",               \"SLA\":\"90 days\"},\n",
    "])\n",
    "display(triage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing function/API for new points\n",
    "# Build a core-sample router for DBSCAN (predict-like)\n",
    "if label_col == \"cluster_db\":\n",
    "    est = DBSCAN(eps=eps, min_samples=10, n_jobs=-1).fit(X if isinstance(X, np.ndarray) else X.values)\n",
    "    core_idx = est.core_sample_indices_ if hasattr(est, \"core_sample_indices_\") else np.array([], dtype=int)\n",
    "    if core_idx.size > 0:\n",
    "        core_labels = est.labels_[core_idx]\n",
    "        nn = NearestNeighbors(n_neighbors=1).fit((X if isinstance(X, np.ndarray) else X.values)[core_idx])\n",
    "    else:\n",
    "        core_labels = np.array([]); nn = None\n",
    "\n",
    "    def route_dbscan(x_new, max_dist=None):\n",
    "        \"\"\"\n",
    "        Assign new point to the nearest core sample's cluster; if farther than max_dist (or no cores), return -1 (noise).\n",
    "        x_new: array-like shape (n_features,)\n",
    "        \"\"\"\n",
    "        if nn is None or core_idx.size == 0:\n",
    "            return -1\n",
    "        d, idx = nn.kneighbors(np.asarray(x_new).reshape(1,-1), n_neighbors=1, return_distance=True)\n",
    "        if (max_dist is not None) and (d[0,0] > max_dist):\n",
    "            return -1\n",
    "        return int(core_labels[idx[0,0]])\n",
    "\n",
    "    # Example usage:\n",
    "    example_label = route_dbscan((X[0] if isinstance(X, np.ndarray) else X.values[0]), max_dist=eps)\n",
    "    print(\"DBSCAN router example label:\", example_label)\n",
    "else:\n",
    "    # HDBSCAN: use approximate_predict if available\n",
    "    try:\n",
    "        import hdbscan\n",
    "        from hdbscan.prediction import approximate_predict\n",
    "        def route_hdbscan(x_new):\n",
    "            \"\"\"\n",
    "            Assign via HDBSCAN approximate_predict; returns (label, strength).\n",
    "            \"\"\"\n",
    "            lab, strength = approximate_predict(hdb, np.asarray(x_new).reshape(1,-1))\n",
    "            return int(lab[0]), float(strength[0])\n",
    "        lab_ex, s_ex = route_hdbscan(X[0] if isinstance(X, np.ndarray) else X.values[0])\n",
    "        print(\"HDBSCAN router example:\", lab_ex, \"strength:\", round(s_ex,3))\n",
    "    except Exception as e:\n",
    "        print(\"HDBSCAN routing not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness audit focused on NOISE and playbook exposure\n",
    "# Pick candidate segment columns (low-cardinality categoricals)\n",
    "cand = [c for c in df.columns if df[c].dtype == 'object' or pd.api.types.is_categorical_dtype(df[c])]\n",
    "cand += [c for c in df.columns if df[c].nunique() <= 8 and c not in [label_col] and c not in cand]\n",
    "cand = list(dict.fromkeys(cand))  # dedupe, keep order\n",
    "\n",
    "def segment_table(col):\n",
    "    g = df.groupby(col, dropna=False)\n",
    "    out = g.apply(lambda s: pd.Series({\n",
    "        \"n\": len(s),\n",
    "        \"noise_rate\": np.mean(s[label_col] == -1),\n",
    "        \"clustered_rate\": np.mean(s[label_col] != -1),\n",
    "    }))\n",
    "    return out.sort_values(\"noise_rate\", ascending=False)\n",
    "\n",
    "audits = {}\n",
    "for c in cand[:6]:  # limit to a few\n",
    "    try:\n",
    "        tab = segment_table(c)\n",
    "        audits[c] = tab\n",
    "        print(f\"\\n== Segment: {c} ==\")\n",
    "        display(tab)\n",
    "        # Simple flag if disparity > 10 percentage points between max and min noise_rate\n",
    "        if tab[\"noise_rate\"].max() - tab[\"noise_rate\"].min() > 0.10:\n",
    "            print(\"Disparity > 10pp in noise rate across segments — review routing thresholds/features.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Optional: list which segments are over/under-exposed to each cluster playbook (excluding noise)\n",
    "if \"name\" in summaries.columns:\n",
    "    exposed = df[df[label_col] != -1].merge(summaries[[\"label\",\"name\"]], left_on=label_col, right_on=\"label\", how=\"left\")\n",
    "    for c in cand[:4]:\n",
    "        try:\n",
    "            share = exposed.pivot_table(index=c, columns=\"name\", values=\"label\", aggfunc='count', fill_value=0)\n",
    "            share = (share.T / share.sum(axis=1)).T  # normalize per segment\n",
    "            print(f\"\\nPlaybook exposure by segment: {c}\")\n",
    "            display(share.round(3))\n",
    "        except Exception:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
