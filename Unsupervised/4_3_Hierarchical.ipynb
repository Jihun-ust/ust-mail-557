{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcf47a4",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Explore agglomerative clustering with Ward and Average linkage. Show a truncated dendrogram and cut at a chosen distance/k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Unsupervised/unsup_utils.py.py\n",
    "import unsup_utils as utils\n",
    "csv_path = \"https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Unsupervised/unsup.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "X, cols, sc = utils.feature_matrix(df, use_emb=True)\n",
    "\n",
    "# Subsample for dendrogram readability\n",
    "sub = min(600, X.shape[0])\n",
    "Xsub = X[:sub]\n",
    "\n",
    "# Dendrogram (using scipy if available)\n",
    "try:\n",
    "    from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "    Z = linkage(Xsub, method=\"ward\")\n",
    "    plt.figure(figsize=(10,4)); dendrogram(Z, truncate_mode=\"lastp\", p=20, no_labels=True); plt.title(\"Truncated dendrogram (Ward)\"); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"scipy not available for dendrogram; skipping plot.\")\n",
    "\n",
    "# Agglomerative with chosen k\n",
    "ward = AgglomerativeClustering(n_clusters=4, linkage=\"ward\")\n",
    "labels = ward.fit_predict(X)\n",
    "df[\"cluster_ward\"] = labels\n",
    "\n",
    "X2, p = utils.pca_2d(X)\n",
    "utils.plot_xy(X2, title=\"PCA (colored by Ward clusters)\", labels=df[\"cluster_ward\"].values)\n",
    "pd.crosstab(df[\"cluster_ward\"], df[\"doc_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest coarse/fine cuts: use big “elbows” in linkage heights\n",
    "heights = Z[:,2]\n",
    "# Heuristic: coarse at ~95th pct height; fine at ~85th pct height\n",
    "h_coarse = np.percentile(heights, 95)\n",
    "h_fine   = np.percentile(heights, 85)\n",
    "# Convert heights to cluster labels on the subsample\n",
    "labs_coarse = fcluster(Z, t=h_coarse, criterion=\"distance\")\n",
    "labs_fine   = fcluster(Z, t=h_fine,   criterion=\"distance\")\n",
    "print(f\"Coarse cut @ height≈{h_coarse:.2f} → k={len(np.unique(labs_coarse))}\")\n",
    "print(f\"Fine   cut @ height≈{h_fine:.2f} → k={len(np.unique(labs_fine))}\")\n",
    "\n",
    "# Simple rationale text\n",
    "print(\"Rationale: Coarse cut targets large merges near the top (big height jump) to yield few, robust clusters;\")\n",
    "print(\"Fine cut sets a lower height to capture secondary structure without over-fragmenting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster sizes & plain-language summaries\n",
    "cluster_col = \"cluster_ward\"\n",
    "sizes = df[cluster_col].value_counts().sort_index()\n",
    "display(pd.DataFrame({\"cluster\": sizes.index, \"size\": sizes.values}))\n",
    "\n",
    "# Numeric columns only, but drop any that start with \"emb\"\n",
    "non_feat = set([cluster_col, \"doc_type\", \"ts\", \"id\"])\n",
    "num_cols = [\n",
    "    c for c in df.columns\n",
    "    if c not in non_feat\n",
    "    and np.issubdtype(df[c].dtype, np.number)\n",
    "    and not c.startswith(\"emb\")\n",
    "]\n",
    "\n",
    "if len(num_cols) == 0:\n",
    "    print(\"No usable numeric columns for summaries; skipping.\")\n",
    "else:\n",
    "    scaler = StandardScaler().fit(df[num_cols])\n",
    "    Znum = pd.DataFrame(scaler.transform(df[num_cols]), columns=num_cols, index=df.index)\n",
    "\n",
    "    summary_rows = []\n",
    "    for k in sorted(df[cluster_col].unique()):\n",
    "        idx = df[cluster_col] == k\n",
    "        mean_z = Znum[idx].mean().sort_values(ascending=False)\n",
    "        top_pos = mean_z.head(3).index.tolist()\n",
    "        top_neg = mean_z.tail(3).index.tolist()\n",
    "        summary = f\"↑ {', '.join(top_pos)} | ↓ {', '.join(top_neg)}\"\n",
    "        summary_rows.append({\n",
    "            \"cluster\": k,\n",
    "            \"size\": int(idx.sum()),\n",
    "            \"summary\": summary\n",
    "        })\n",
    "\n",
    "    summaries = pd.DataFrame(summary_rows).sort_values(\"cluster\").reset_index(drop=True)\n",
    "    display(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal validity & cophenetic correlation\n",
    "# Compute on full X with current labels\n",
    "try:\n",
    "    sil = silhouette_score(X, df[cluster_col].values)\n",
    "except Exception:\n",
    "    sil = np.nan\n",
    "db  = davies_bouldin_score(X, df[cluster_col].values)\n",
    "ch  = calinski_harabasz_score(X, df[cluster_col].values)\n",
    "print(f\"Silhouette: {sil:.3f}  (higher is better)\")\n",
    "print(f\"Davies-Bouldin: {db:.3f}  (lower is better)\")\n",
    "print(f\"Calinski–Harabasz: {ch:.1f}  (higher is better)\")\n",
    "\n",
    "# Cophenetic correlation (how well the tree preserves distances)\n",
    "D = pdist(Xsub)\n",
    "Z_full = linkage(Xsub, method=\"ward\")\n",
    "coph_corr, coph_dists = cophenet(Z_full, D)\n",
    "print(f\"Cophenetic correlation (Ward/tree fidelity on subsample): {coph_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster names & playbooks (auto, using summaries w/out emb*)\n",
    "def make_name(summary_text: str) -> str:\n",
    "    \"\"\"Pick first two 'up' features from summary as cluster name.\"\"\"\n",
    "    ups = [t.strip() for t in summary_text.split(\"|\")[0].replace(\"↑\", \"\").split(\",\")]\n",
    "    ups = [u for u in ups if u][:2]\n",
    "    return \" & \".join(ups) if ups else \"Cluster\"\n",
    "\n",
    "def make_playbook(summary_text: str) -> str:\n",
    "    \"\"\"Generate a lightweight playbook guidance from top/bottom features.\"\"\"\n",
    "    ups = [t.strip() for t in summary_text.split(\"|\")[0].replace(\"↑\", \"\").split(\",\")]\n",
    "    downs = [t.strip() for t in summary_text.split(\"|\")[-1].replace(\"↓\", \"\").split(\",\")]\n",
    "    ups = [u for u in ups if u][:3]\n",
    "    downs = [d for d in downs if d][:2]\n",
    "\n",
    "    return (\n",
    "        f\"Lean into {', '.join(ups)}; \"\n",
    "        f\"watch for {', '.join(downs)}. \"\n",
    "        \"Use this cluster’s signature to tailor campaigns or ops playbooks.\"\n",
    "    )\n",
    "\n",
    "# Build cards from the summaries DataFrame (already excludes 'emb*')\n",
    "cards = []\n",
    "for _, row in summaries.iterrows():\n",
    "    name = make_name(row[\"summary\"])\n",
    "    play = make_playbook(row[\"summary\"])\n",
    "    cards.append({\n",
    "        \"cluster\": int(row[\"cluster\"]),\n",
    "        \"name\": name,\n",
    "        \"size\": int(row[\"size\"]),\n",
    "        \"summary\": row[\"summary\"],\n",
    "        \"playbook\": play\n",
    "    })\n",
    "\n",
    "cluster_cards = pd.DataFrame(cards).sort_values(\"cluster\").reset_index(drop=True)\n",
    "display(cluster_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly policy: long branches & late joins\n",
    "# Heuristic policy:\n",
    "#  - Low silhouette samples (e.g., bottom 5%) within each cluster → “ambiguous membership”\n",
    "#  - Large distance to cluster centroid in PCA space (top 2%) → “outliers / late joins”\n",
    "# PCA to 2D for distance intuition\n",
    "X2, _ = utils.pca_2d(X)\n",
    "cent = NearestCentroid().fit(X2, df[cluster_col].values)\n",
    "d2 = ((X2 - cent.centroids_[df[cluster_col].values])**2).sum(axis=1)**0.5\n",
    "\n",
    "# Compute per-sample silhouette (falls back if k<2)\n",
    "try:\n",
    "    from sklearn.metrics import silhouette_samples\n",
    "    sil_samp = silhouette_samples(X, df[cluster_col].values)\n",
    "except Exception:\n",
    "    sil_samp = np.full(X.shape[0], np.nan)\n",
    "\n",
    "th_low_sil = np.nanpercentile(sil_samp, 5)\n",
    "th_hi_dist = np.nanpercentile(d2, 98)\n",
    "\n",
    "anomalies = pd.DataFrame({\n",
    "    \"low_silhouette\": sil_samp < th_low_sil,\n",
    "    \"far_from_centroid\": d2 > th_hi_dist,\n",
    "}, index=df.index)\n",
    "anomalies[\"policy_flag\"] = anomalies.any(axis=1)\n",
    "\n",
    "print(f\"Policy thresholds → silhouette < {th_low_sil:.3f}, PCA-distance > {th_hi_dist:.3f}\")\n",
    "print(\"Flag rate:\", anomalies[\"policy_flag\"].mean().round(4))\n",
    "display(df.loc[anomalies[\"policy_flag\"], [cluster_col, \"doc_type\"]].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
