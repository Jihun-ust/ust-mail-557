{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b9fca7",
   "metadata": {},
   "source": [
    "# Polynomial Fits: Compare Degrees 1–4\n",
    "Generate a synthetic dataset with a clearly non-linear relationship, fit **polynomial regressions** of degrees **1, 2, 3, and 4**, and compare performance and visuals.\n",
    "\n",
    "**What you’ll see**\n",
    "- A reproducible dataset with slight outliers to highlight under/overfitting differences.\n",
    "- Train/test evaluation across polynomial degrees 1–4.\n",
    "- Clear plots overlaying the fits on the data.\n",
    "- A short interpretation guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & global style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Clean & modern\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "plt.rcParams['font.size'] = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935cd7cc",
   "metadata": {},
   "source": [
    "## 1) Generate Data\n",
    "Create a clearly non-linear signal (approximately cubic) with Gaussian noise and a few outliers to make differences between low and high polynomial degrees more evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible data\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "n = 120 # total samples\n",
    "x = rng.uniform(-3.0, 3.0, size=n)\n",
    "# True function: mostly cubic with additional structure\n",
    "def true_fn(t):\n",
    "    return 0.6*t**3 - 1.0*t**2 + 2.2*t + 1.0\n",
    "\n",
    "noise = rng.normal(0, 2.5, size=n)  # base noise\n",
    "y = true_fn(x) + noise\n",
    "\n",
    "# Add a few outliers (to emphasize overfitting at higher degrees)\n",
    "n_outliers = max(4, n // 25)  # ~4-6 outliers\n",
    "idx_out = rng.choice(np.arange(n), size=n_outliers, replace=False)\n",
    "y[idx_out] += rng.normal(0, 10.0, size=n_outliers)\n",
    "\n",
    "# Train/test split\n",
    "X = x.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "\n",
    "print(f\"Samples: {n}   Train: {len(X_train)}   Test: {len(X_test)}   Outliers: {n_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0537f4",
   "metadata": {},
   "source": [
    "### Quick look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ce435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_train[:, 0], y_train, s=16, alpha=0.8, label='train points')\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f62831",
   "metadata": {},
   "source": [
    "## 2) Fit & Evaluate Polynomial Models (Degrees 1–4)\n",
    "Fit four models using a scikit-learn pipeline: `PolynomialFeatures(degree=d)` + `LinearRegression()`.\n",
    "Report **MSE** and **R²** on both train and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(degree, X_train, y_train, X_test, y_test):\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('linreg', LinearRegression())\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat_train = model.predict(X_train)\n",
    "    yhat_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'degree': degree,\n",
    "        'train_MSE': mean_squared_error(y_train, yhat_train),\n",
    "        'test_MSE': mean_squared_error(y_test, yhat_test),\n",
    "        'train_R2': r2_score(y_train, yhat_train),\n",
    "        'test_R2': r2_score(y_test, yhat_test),\n",
    "        'model': model\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "results = []\n",
    "models = {}\n",
    "for d in [1, 2, 3, 4]:\n",
    "    m = fit_poly(d, X_train, y_train, X_test, y_test)\n",
    "    results.append({k:v for k,v in m.items() if k != 'model'})\n",
    "    models[d] = m['model']\n",
    "\n",
    "df = pd.DataFrame(results).sort_values('degree').reset_index(drop=True)\n",
    "df_display = df[['degree','train_MSE','test_MSE','train_R2','test_R2']]\n",
    "df_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4c977",
   "metadata": {},
   "source": [
    "## 3) Visual Comparison\n",
    "Overlay the fitted curves for degrees 1–4 on a dense grid. Differences should be visible:\n",
    "- **Degree 1–2** underfit the cubic trend.\n",
    "- **Degree 3** matches the underlying pattern best.\n",
    "- **Degree 4** may start to overfit, especially with outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55998b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense grid for smooth curves\n",
    "x_grid = np.linspace(X.min(), X.max(), 400).reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:,0], y_train, s=12, alpha=0.6, label='train points')\n",
    "# Plot each degree fit\n",
    "for d in [1,2,3,4]:\n",
    "    y_grid = models[d].predict(x_grid)\n",
    "    # Show test R2 in legend to emphasize generalization\n",
    "    r2 = df_display.loc[df_display['degree']==d, 'test_R2'].iloc[0]\n",
    "    plt.plot(x_grid[:,0], y_grid, linewidth=2, label=f'degree {d} (test R²={r2:.3f})')\n",
    "\n",
    "plt.title('Polynomial Fits (Degrees 1–4)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e088890",
   "metadata": {},
   "source": [
    "### Optional: Residual View per Degree\n",
    "Inspect residuals (errors) for each degree on the **test set**. Lower spread around 0 with no clear pattern usually indicates a better fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [1,2,3,4]:\n",
    "    yhat_test = models[d].predict(X_test)\n",
    "    residuals = y_test - yhat_test\n",
    "    plt.figure()\n",
    "    plt.scatter(X_test[:,0], residuals, s=16, alpha=0.8, label=f'residuals (deg {d})')\n",
    "    plt.axhline(0, linestyle='--', linewidth=1)\n",
    "    plt.title(f'Residuals vs x (Degree {d})')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('test residual')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea80dd5",
   "metadata": {},
   "source": [
    "## 4) What to Notice\n",
    "- **Underfitting**: Degrees **1** and sometimes **2** cannot capture the curvature of the true signal → lower R² and higher error.\n",
    "- **Right capacity**: Degree **3** matches the (approximate) data-generating process → best or near-best test performance.\n",
    "- **Overfitting risk**: Degree **4** can **chase outliers/noise**, sometimes improving train fit but hurting test metrics.\n",
    "- When you change the random seed or noise level, you may see the gap widen or narrow—**the pattern remains**: match model complexity to signal complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19370ae6",
   "metadata": {},
   "source": [
    "### Try it: Change parameters\n",
    "You can tweak `n`, noise levels, or the number of outliers and re-run the notebook to observe how the comparison changes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
