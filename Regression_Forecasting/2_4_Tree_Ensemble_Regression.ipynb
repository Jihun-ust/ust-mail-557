{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648259de",
   "metadata": {},
   "source": [
    "# Tree-Based & Ensemble Regression\n",
    "\n",
    "Compare a CART-style Decision Tree, Random Forest, and Gradient Boosting on the same scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03fb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay, partial_dependence\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.base import clone\n",
    "from numpy.random import RandomState\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Regression_Forecasting/reg_for_utils.py\n",
    "import reg_for_utils as utils\n",
    "csv_path = \"https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Regression_Forecasting/marketing_daily.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "train, test = utils.time_train_test_split(df, \"date\", test_days=90)\n",
    "\n",
    "features = [\"search_spend\",\"social_spend\",\"display_spend\",\"promo\",\"price_index\",\"temp_F\",\"rain\",\"is_weekend\"]\n",
    "target = \"revenue\"\n",
    "X_train, y_train = train[features], train[target]\n",
    "X_test, y_test = test[features], test[target]\n",
    "\n",
    "pre = ColumnTransformer([(\"num\", StandardScaler(), [\"search_spend\",\"social_spend\",\"display_spend\",\"price_index\",\"temp_F\"]),\n",
    "                         (\"cat\", OneHotEncoder(drop=\"if_binary\"), [\"promo\",\"rain\",\"is_weekend\"])])\n",
    "\n",
    "models = {\n",
    "    \"tree\": Pipeline([(\"pre\", pre), (\"est\", DecisionTreeRegressor(max_depth=6, random_state=42))]),\n",
    "    \"rf\": Pipeline([(\"pre\", pre), (\"est\", RandomForestRegressor(n_estimators=300, max_depth=6, random_state=42, n_jobs=-1))]),\n",
    "    \"gbr\": Pipeline([(\"pre\", pre), (\"est\", GradientBoostingRegressor(random_state=42))])\n",
    "}\n",
    "\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)\n",
    "    y_pred = m.predict(X_test)\n",
    "    print(name.UPPER() if hasattr(name, \"UPPER\") else name.upper(), \"RMSE:\", utils.rmse(y_test, y_pred), \"MAE:\", utils.mae(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e020d8",
   "metadata": {},
   "source": [
    "## Feature importance (Random Forest) — intuition for stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = models[\"rf\"].named_steps[\"est\"]\n",
    "feat_names = models[\"rf\"].named_steps[\"pre\"].get_feature_names_out()\n",
    "importances = rf.feature_importances_\n",
    "imp = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "imp.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d052d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = models[\"tree\"].named_steps[\"est\"]\n",
    "feat_names = models[\"tree\"].named_steps[\"pre\"].get_feature_names_out()\n",
    "importances = tree.feature_importances_\n",
    "imp = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "imp.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9eae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = models[\"gbr\"].named_steps[\"est\"]\n",
    "feat_names = models[\"gbr\"].named_steps[\"pre\"].get_feature_names_out()\n",
    "importances = gbr.feature_importances_\n",
    "imp = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "imp.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588b8cd",
   "metadata": {},
   "source": [
    "### (Optional) Diagnose Examples\n",
    "- Permutation Importances\n",
    "- Partial Dependence (PDP) & Individual Conditional Expectation (ICE)\n",
    "- Residual Diagnostics\n",
    "- Overfitting Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which fitted model to diagnose\n",
    "MODEL_KEY = \"rf\"  # one of: \"tree\", \"rf\", \"gbr\"\n",
    "estimator = models[MODEL_KEY]\n",
    "\n",
    "X_tr, y_tr = X_train, y_train\n",
    "X_va, y_va = X_test, y_test   # treat test as \"validation\" for diagnostics\n",
    "\n",
    "def rmse(y_true, y_pred): \n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def final_est(est):\n",
    "    return est.steps[-1][1] if isinstance(est, Pipeline) else est\n",
    "\n",
    "def param_key(name):  # for your pipelines where final step is named \"est\"\n",
    "    return f\"est__{name}\"\n",
    "\n",
    "def feature_names():  # original columns (preprocessor handles transforms internally)\n",
    "    return list(X_tr.columns)\n",
    "\n",
    "print(f\"Diagnosing model: {MODEL_KEY}  |  features: {feature_names()}\")\n",
    "print(\"Validation metrics → RMSE: %.3f | MAE: %.3f | R2: %.3f\" % (\n",
    "    rmse(y_va, estimator.predict(X_va)),\n",
    "    mean_absolute_error(y_va, estimator.predict(X_va)),\n",
    "    r2_score(y_va, estimator.predict(X_va)),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b088f9",
   "metadata": {},
   "source": [
    "### Permutation Importances\n",
    "\n",
    "- **Observation:** Each feature’s importance is plotted with a point (mean importance) and error bars (95% confidence interval from bootstrapping).\n",
    "- **Interpretation:**\n",
    "  - A **larger mean importance** means the model’s predictions degrade more when that feature is randomly shuffled — i.e., the feature carries more signal.\n",
    "  - **Wide error bars** suggest uncertainty: the feature’s importance varies depending on which rows were resampled.\n",
    "  - **Near-zero importances** (and CIs that cross zero) mean the model isn’t really using that feature for predictive power.\n",
    "- **Leadership takeaway:** This lets you rank which business drivers (e.g., search vs. social spend, weather, price index) the model relies on most, and whether the signal is robust or fragile.\n",
    "\n",
    "- **Note**: Permutation importances can take much longer to execute than other diagnostics because it repeatedly retrains on resampled data and shuffles each feature many times, multiplying computation cost across features and bootstraps.\n",
    "  - FYI: Estimated time of executions\n",
    "    - Basic runtime: **approx. 1 hour** (Google Colab free tier, typically 2-core vCPU, K80 or T4 GPU)\n",
    "    - Better runtime: **approx. 6 minutes** (Tested on 10-core CPU, 10-core GPU, 16-core Neural Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importances (with bootstrap 95% CI) — FIXED RNG\n",
    "SCORING = \"neg_root_mean_squared_error\"  # RMSE-oriented\n",
    "\n",
    "def permutation_importance_ci(est, X, y, n_repeats=40, n_boot=200, random_state=42):\n",
    "    rng = RandomState(random_state)  # <- use RandomState, not default_rng/Generator\n",
    "    feats = feature_names()\n",
    "    imps = []\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.randint(0, len(X), size=len(X))  # bootstrap indices\n",
    "        Xb = X.iloc[idx]\n",
    "        yb = y.iloc[idx]\n",
    "        # pass RandomState (or an int) to sklearn\n",
    "        res = permutation_importance(\n",
    "            est, Xb, yb,\n",
    "            n_repeats=n_repeats,\n",
    "            scoring=SCORING,\n",
    "            random_state=rng,   # OK: RandomState instance\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        imps.append(res.importances_mean)\n",
    "\n",
    "    imp = np.vstack(imps)\n",
    "    df_imp = pd.DataFrame({\n",
    "        \"feature\": feats,\n",
    "        \"importance_mean\": imp.mean(axis=0),\n",
    "        \"ci_lo\": np.percentile(imp, 2.5, axis=0),\n",
    "        \"ci_hi\": np.percentile(imp, 97.5, axis=0),\n",
    "    }).sort_values(\"importance_mean\", ascending=False).reset_index(drop=True)\n",
    "    return df_imp\n",
    "\n",
    "perm_df = permutation_importance_ci(estimator, X_va, y_va, n_repeats=40, n_boot=200, random_state=42)\n",
    "display(perm_df)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, max(3, 0.45*len(perm_df))))\n",
    "ax.errorbar(perm_df[\"importance_mean\"][::-1], np.arange(len(perm_df))[::-1],\n",
    "            xerr=[perm_df[\"importance_mean\"][::-1] - perm_df[\"ci_lo\"][::-1],\n",
    "                  perm_df[\"ci_hi\"][::-1] - perm_df[\"importance_mean\"][::-1]],\n",
    "            fmt=\"o\", capsize=3)\n",
    "ax.set_yticks(np.arange(len(perm_df))[::-1])\n",
    "ax.set_yticklabels(perm_df[\"feature\"][::-1])\n",
    "ax.set_xlabel(\"Permutation importance (mean; 95% CI)\")\n",
    "ax.set_title(f\"Permutation Importances with 95% CI — {MODEL_KEY.upper()} (Validation)\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae64bc",
   "metadata": {},
   "source": [
    "### Partial Dependence (PDP) & Individual Conditional Expectation (ICE)\n",
    "\n",
    "- **PDP (average curves):**\n",
    "  - Shows the **average relationship** between a single feature and predicted revenue, holding other features constant.\n",
    "  - Example: If the curve slopes upward for \"search_spend,\" it means higher spend tends to increase predicted revenue on average.\n",
    "- **ICE (individual lines):**\n",
    "  - Each thin line is one data point’s predicted revenue as the feature varies across a grid.\n",
    "  - Divergence among lines shows **heterogeneity**: the feature’s effect depends on context (interactions with other variables).\n",
    "- **Slope annotations:** Mark key ranges (e.g., 5k→20k spend, 30→80°F) and calculate the model-implied marginal effect. Steeper slopes = larger returns for changes in that range.\n",
    "- **Leadership takeaway:** PDP/ICE curves help translate model mechanics into **business intuition** (e.g., where marketing dollars are most effective, or when weather starts to affect demand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6782f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDP & ICE for top features (manual, version‑proof) + slope tags\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, warnings\n",
    "from numpy.random import RandomState\n",
    "\n",
    "TOP_K = 4  # change as desired\n",
    "rng = RandomState(42)\n",
    "\n",
    "# Use the permutation importance ranking from Cell 1\n",
    "top_feats = list(perm_df[\"feature\"][:TOP_K])\n",
    "print(\"Top features for PDP/ICE:\", top_feats)\n",
    "\n",
    "# Optional slope annotation ranges—edit if these columns exist in your data\n",
    "SLOPE_MARKS = {\n",
    "    \"search_spend\": [(5_000, 20_000)],\n",
    "    \"social_spend\": [(5_000, 20_000)],\n",
    "    \"display_spend\": [(5_000, 20_000)],\n",
    "    \"temp_F\": [(30, 80)],\n",
    "}\n",
    "\n",
    "def get_feature_grid(X, feat, n=50):\n",
    "    \"\"\"Make a sensible grid for PDP depending on feature type.\"\"\"\n",
    "    x = X[feat].dropna()\n",
    "    uniq = np.unique(x)\n",
    "    # Treat small‑cardinality features (incl. binaries) as categorical grid\n",
    "    if len(uniq) <= 5:\n",
    "        return np.sort(uniq.astype(float))\n",
    "    # Robust numeric grid between 1st and 99th percentile\n",
    "    lo, hi = np.percentile(x, [1, 99])\n",
    "    if lo == hi:\n",
    "        lo, hi = x.min(), x.max()\n",
    "    if lo == hi:\n",
    "        return np.array([float(lo)])\n",
    "    return np.linspace(lo, hi, n)\n",
    "\n",
    "def pdp_manual(est, X, feat, grid):\n",
    "    \"\"\"\n",
    "    Classic PDP: for each grid value v, set X[feat]=v for all rows,\n",
    "    predict, and average the predictions.\n",
    "    Works with Pipelines that include preprocessing.\n",
    "    \"\"\"\n",
    "    avgs = []\n",
    "    for v in grid:\n",
    "        Xv = X.copy()\n",
    "        Xv[feat] = v\n",
    "        yhat = est.predict(Xv)\n",
    "        avgs.append(np.mean(yhat))\n",
    "    return np.asarray(avgs)\n",
    "\n",
    "def ice_manual(est, X_sample, feat, grid):\n",
    "    \"\"\"\n",
    "    ICE: for each grid value v, set X_sample[feat]=v and predict per row.\n",
    "    Returns matrix shape (n_rows, len(grid)).\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for v in grid:\n",
    "        Xv = X_sample.copy()\n",
    "        Xv[feat] = v\n",
    "        yh = est.predict(Xv)\n",
    "        lines.append(yh)\n",
    "    M = np.column_stack(lines)  # n_rows × len(grid)\n",
    "    return M\n",
    "\n",
    "def annotate_slope(ax, grid, avg, x0, x1, label):\n",
    "    i0 = int(np.argmin(np.abs(grid - x0)))\n",
    "    i1 = int(np.argmin(np.abs(grid - x1)))\n",
    "    dy = float(avg[i1] - avg[i0])\n",
    "    dx = float(grid[i1] - grid[i0]) if grid[i1] != grid[i0] else np.nan\n",
    "    slope = dy/dx if (dx and not np.isnan(dx)) else np.nan\n",
    "    ax.plot([grid[i0], grid[i1]], [avg[i0], avg[i1]], lw=2)\n",
    "    ax.scatter([grid[i0], grid[i1]], [avg[i0], avg[i1]], s=35)\n",
    "    ax.annotate(f\"{label}\\nΔy={dy:.2f}, Δx={dx:.2f}\\nslope={slope:.4f}\",\n",
    "                xy=(grid[i1], avg[i1]), xytext=(6,6), textcoords=\"offset points\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"w\", ec=\"0.5\", alpha=0.9))\n",
    "\n",
    "for feat in top_feats:\n",
    "    # Build grid\n",
    "    grid = get_feature_grid(X_tr, feat, n=60)\n",
    "\n",
    "    # PDP (average)\n",
    "    avg = pdp_manual(estimator, X_tr, feat, grid)\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    ax.plot(grid, avg, linewidth=2)\n",
    "    ax.set_xlabel(feat); ax.set_ylabel(\"Predicted revenue\")\n",
    "    ax.set_title(f\"PDP (average) — {feat}  [{MODEL_KEY.upper()}]\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ICE (on a subsample to keep it readable)\n",
    "    n_lines = 200 if len(X_tr) > 200 else len(X_tr)\n",
    "    rows = rng.choice(X_tr.index.values, size=n_lines, replace=False)\n",
    "    X_sample = X_tr.loc[rows].copy()\n",
    "    M = ice_manual(estimator, X_sample, feat, grid)  # shape: n_lines × len(grid)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    for i in range(M.shape[0]):\n",
    "        ax.plot(grid, M[i, :], alpha=0.06)\n",
    "    ax.set_xlabel(feat); ax.set_ylabel(\"Predicted revenue\")\n",
    "    ax.set_title(f\"ICE — {feat}  [{MODEL_KEY.upper()}]\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Optional slope annotations on PDP curve at key ranges\n",
    "    ranges = SLOPE_MARKS.get(feat, [])\n",
    "    if len(grid) >= 2 and len(ranges) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        ax.plot(grid, avg, linewidth=2)\n",
    "        ax.set_xlabel(feat); ax.set_ylabel(\"Predicted revenue\")\n",
    "        ax.set_title(f\"PDP with slope annotations — {feat}\")\n",
    "        for (x0, x1) in ranges:\n",
    "            annotate_slope(ax, grid, avg, x0, x1, f\"{x0}→{x1}\")\n",
    "        plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520d086",
   "metadata": {},
   "source": [
    "### Residual Diagnostics\n",
    "\n",
    "- **Residuals vs. Predicted:** \n",
    "  - Scatter plot of prediction error (residual = actual – predicted).\n",
    "  - Ideally, residuals should be centered around zero, with no obvious pattern.\n",
    "  - Patterns (e.g., funnel shape or systematic bias at high predictions) suggest the model misfits certain ranges.\n",
    "- **By Segment (promo / rain / weekend):**\n",
    "  - Boxplots show whether errors are systematically higher or lower in certain categories.\n",
    "  - If residuals are consistently positive under \"promo=1,\" the model underestimates promo effects.\n",
    "- **Over Time:**\n",
    "  - Rolling average of residuals highlights whether accuracy drifts across seasons or months.\n",
    "  - Seasonal boxplots (winter, spring, summer, fall) reveal calendar effects the model may not fully capture.\n",
    "- **Leadership takeaway:** Residual diagnostics reveal **blind spots** — e.g., underpredicting in promotions, overpredicting in rainy weeks, or drift over seasons — which can guide feature engineering or business adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a validation-side frame aligned with X_va rows for segmentation\n",
    "va_df = df.loc[X_va.index, [\"date\",\"promo\",\"rain\",\"is_weekend\"]].copy()\n",
    "va_df[\"y_true\"] = y_va.values\n",
    "va_df[\"y_pred\"] = estimator.predict(X_va)\n",
    "va_df[\"resid\"]  = va_df[\"y_true\"] - va_df[\"y_pred\"]\n",
    "\n",
    "# Residuals vs. predicted\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.scatter(va_df[\"y_pred\"], va_df[\"resid\"], alpha=0.6)\n",
    "ax.axhline(0, linewidth=1, color=\"k\")\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Residual (y - ŷ)\")\n",
    "ax.set_title(f\"Residuals vs Predicted — {MODEL_KEY.upper()} (Validation)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Categorical segments\n",
    "for seg in [\"promo\",\"rain\",\"is_weekend\"]:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    groups = [va_df.loc[va_df[seg]==v, \"resid\"] for v in sorted(va_df[seg].unique())]\n",
    "    ax.boxplot(groups, labels=[str(v) for v in sorted(va_df[seg].unique())])\n",
    "    ax.axhline(0, color=\"k\", lw=1)\n",
    "    ax.set_xlabel(seg); ax.set_ylabel(\"Residual\")\n",
    "    ax.set_title(f\"Residuals by {seg} — {MODEL_KEY.upper()} (Validation)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Over time (rolling mean to spot drift or seasonality)\n",
    "ts = va_df.dropna(subset=[\"date\"]).sort_values(\"date\").copy()\n",
    "ts[\"roll_mean_resid\"] = ts[\"resid\"].rolling(window=8, min_periods=1).mean()  # ~2 months if weekly\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(ts[\"date\"], ts[\"resid\"], alpha=0.25, label=\"residual\")\n",
    "ax.plot(ts[\"date\"], ts[\"roll_mean_resid\"], lw=2, label=\"rolling mean (w=8)\")\n",
    "ax.axhline(0, color=\"k\", lw=1)\n",
    "ax.set_title(f\"Residuals Over Time — {MODEL_KEY.upper()} (Validation)\")\n",
    "ax.set_xlabel(\"date\"); ax.set_ylabel(\"residual\"); ax.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac68d4",
   "metadata": {},
   "source": [
    "### Overfitting Diagnostics\n",
    "\n",
    "- **Train vs. Validation RMSE (grid over depth & trees):**\n",
    "  - **Training RMSE** (dashed lines) shows how well the model fits the training data.\n",
    "  - **Validation RMSE** (solid lines) shows generalization to unseen data.\n",
    "  - When train error drops but validation error rises, that’s **overfitting**.\n",
    "  - Shallow trees (low depth) may **underfit** (both errors high).\n",
    "  - Deeper trees or too many estimators may overfit.\n",
    "- **Early-Stopping Curves (Gradient Boosting only):**\n",
    "  - RMSE plotted against number of boosting iterations.\n",
    "  - Training error declines steadily; validation error typically drops then rises.\n",
    "  - The **best iteration** is where validation RMSE is lowest, use this as a natural stopping point to avoid overfitting.\n",
    "- **Leadership takeaway:** These curves show whether the model is too simple, too complex, or just right, and help set guardrails (e.g., limiting depth or enabling early-stopping) for reliable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "# Grid: n_estimators × max_depth (works for RF & GBR; DT uses max_depth only)\n",
    "def grid_eval(est, Xtr, ytr, Xva, yva,\n",
    "              n_estimators_list=(50, 100, 200, 400),\n",
    "              max_depth_list=(None, 3, 5, 7)):\n",
    "    rows = []\n",
    "    fe = final_est(estimator)\n",
    "    is_rf  = isinstance(fe, RandomForestRegressor)\n",
    "    is_gbr = isinstance(fe, GradientBoostingRegressor)\n",
    "    is_dt  = isinstance(fe, DecisionTreeRegressor)\n",
    "\n",
    "    for d in max_depth_list:\n",
    "        for n in n_estimators_list if (is_rf or is_gbr) else [None]:\n",
    "            mdl = clone(estimator)\n",
    "            params = {}\n",
    "            if d is not None:\n",
    "                params[param_key(\"max_depth\")] = d\n",
    "            if n is not None and (is_rf or is_gbr):\n",
    "                params[param_key(\"n_estimators\")] = n\n",
    "            if params:\n",
    "                mdl.set_params(**params)\n",
    "\n",
    "            mdl.fit(Xtr, ytr)\n",
    "            ytr_hat = mdl.predict(Xtr)\n",
    "            yva_hat = mdl.predict(Xva)\n",
    "            rows.append({\n",
    "                \"model\": type(final_est(mdl)).__name__,\n",
    "                \"max_depth\": d,\n",
    "                \"n_estimators\": n if n is not None else (mdl.get_params().get(param_key(\"n_estimators\")) if (is_rf or is_gbr) else None),\n",
    "                \"RMSE_train\": rmse(ytr, ytr_hat),\n",
    "                \"RMSE_val\": rmse(yva, yva_hat),\n",
    "                \"R2_val\": r2_score(yva, yva_hat),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "grid_df = grid_eval(estimator, X_tr, y_tr, X_va, y_va)\n",
    "display(grid_df.sort_values(\"RMSE_val\").head(10))\n",
    "\n",
    "# Plot curves vs n_estimators for each depth (only RF/GBR)\n",
    "fe = final_est(estimator)\n",
    "if isinstance(fe, (RandomForestRegressor, GradientBoostingRegressor)):\n",
    "    fig, ax = plt.subplots(figsize=(9,5))\n",
    "    for d, g in grid_df.groupby(\"max_depth\"):\n",
    "        g = g.dropna(subset=[\"n_estimators\"]).sort_values(\"n_estimators\")\n",
    "        if g.empty: \n",
    "            continue\n",
    "        ax.plot(g[\"n_estimators\"], g[\"RMSE_val\"], marker=\"o\", label=f\"val depth={d}\")\n",
    "        ax.plot(g[\"n_estimators\"], g[\"RMSE_train\"], linestyle=\"--\", marker=\"x\", alpha=0.6, label=f\"train depth={d}\")\n",
    "    ax.set_xlabel(\"n_estimators\"); ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_title(f\"Train vs. Val RMSE across trees & depths — {MODEL_KEY.upper()}\")\n",
    "    ax.legend(ncol=2); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Early‑stopping style curves (only for GBR)\n",
    "if isinstance(fe, GradientBoostingRegressor):\n",
    "    # Refit a cloned model with more estimators to get smooth staged curves\n",
    "    gbr = clone(estimator).set_params(**{param_key(\"n_estimators\"): 600})\n",
    "    gbr.fit(X_tr, y_tr)\n",
    "\n",
    "    # staged_predict runs on the estimator itself (pipeline OK in sklearn≥1.0)\n",
    "    tr_curve = []\n",
    "    va_curve = []\n",
    "    # Extract the final estimator inside the pipeline to iterate staged_predict robustly\n",
    "    # while feeding transformed data:\n",
    "    preproc = gbr.named_steps[\"pre\"]\n",
    "    est = gbr.named_steps[\"est\"]\n",
    "    Xtr_t = preproc.transform(X_tr)\n",
    "    Xva_t = preproc.transform(X_va)\n",
    "\n",
    "    for yhat in est.staged_predict(Xtr_t):\n",
    "        tr_curve.append(rmse(y_tr, yhat))\n",
    "    for yhat in est.staged_predict(Xva_t):\n",
    "        va_curve.append(rmse(y_va, yhat))\n",
    "    iters = np.arange(1, len(tr_curve)+1)\n",
    "    best_iter = int(np.argmin(va_curve) + 1)\n",
    "    print(f\"GBR early‑stopping proxy → best_iter={best_iter}, val RMSE={va_curve[best_iter-1]:.3f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9,5))\n",
    "    ax.plot(iters, tr_curve, label=\"Train RMSE\")\n",
    "    ax.plot(iters, va_curve, label=\"Validation RMSE\")\n",
    "    ax.axvline(best_iter, color=\"k\", linestyle=\"--\", label=f\"best={best_iter}\")\n",
    "    ax.set_xlabel(\"Boosting iterations\"); ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_title(\"Gradient Boosting — Early‑Stopping Curves\")\n",
    "    ax.legend(); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    warnings.warn(\"Early‑stopping curves skipped (final estimator is not GradientBoostingRegressor).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
