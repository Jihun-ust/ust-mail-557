{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabf0187",
   "metadata": {},
   "source": [
    "# Quantile Regression\n",
    "\n",
    "Estimate conditional quantiles (e.g., median and P90) of revenue to understand upside/downside risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d251f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Regression_Forecasting/reg_for_utils.py\n",
    "import reg_for_utils as utils\n",
    "csv_path = \"https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Regression_Forecasting/marketing_daily.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "train, test = utils.time_train_test_split(df, \"date\", test_days=90)\n",
    "\n",
    "X_cols = [\"search_spend\",\"social_spend\",\"display_spend\",\"promo\",\"price_index\",\"temp_F\",\"rain\",\"is_weekend\"]\n",
    "y_col = \"revenue\"\n",
    "\n",
    "pre = ColumnTransformer([(\"num\", StandardScaler(), [\"search_spend\",\"social_spend\",\"display_spend\",\"price_index\",\"temp_F\"]),\n",
    "                         (\"cat\", OneHotEncoder(drop=\"if_binary\"), [\"promo\",\"rain\",\"is_weekend\"])])\n",
    "\n",
    "taus = [0.5, 0.9]\n",
    "models = {}\n",
    "for t in taus:\n",
    "    qr = Pipeline([(\"pre\", pre), (\"est\", QuantileRegressor(quantile=t, alpha=0.0001, solver=\"highs\"))])\n",
    "    qr.fit(train[X_cols], train[y_col])\n",
    "    models[t] = qr\n",
    "    pred = qr.predict(test[X_cols])\n",
    "    loss = mean_pinball_loss(test[y_col], pred, alpha=t)\n",
    "    print(\"tau=\", t, \" pinball_loss=\", round(loss,3), \" MAE=\", round(utils.mae(test[y_col], pred),2))\n",
    "\n",
    "med = models[0.5].predict(test[X_cols])\n",
    "p90 = models[0.9].predict(test[X_cols])\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(test[\"date\"], test[y_col], label=\"actual\")\n",
    "plt.plot(test[\"date\"], med, label=\"median\")\n",
    "plt.plot(test[\"date\"], p90, label=\"p90\")\n",
    "plt.title(\"Revenue: median and P90 quantile regression\"); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2da43b",
   "metadata": {},
   "source": [
    "### Evaluation Examples\n",
    "- Set up predictions & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions & helpers\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "\n",
    "X_cols = X_cols\n",
    "y_col  = y_col\n",
    "\n",
    "# Ensure we have predictions for taus in the 'models' dict\n",
    "preds = {t: models[t].predict(test[X_cols]) for t in models.keys()}\n",
    "y_true = test[y_col].to_numpy()\n",
    "\n",
    "# Combine for analysis\n",
    "eval_df = test[[c for c in test.columns if c in ([\"date\"] + X_cols + [y_col])]].copy()\n",
    "for t, yhat in preds.items():\n",
    "    eval_df[f\"q{int(t*100)}\"] = yhat\n",
    "eval_df[\"y_true\"] = y_true\n",
    "\n",
    "def empirical_coverage(y, qhat):\n",
    "    \"\"\"Fraction of y <= predicted quantile (should be ~ tau).\"\"\"\n",
    "    y = np.asarray(y); qhat = np.asarray(qhat)\n",
    "    return float(np.mean(y <= qhat))\n",
    "\n",
    "def sharpness_width(q_hi, q_lo):\n",
    "    \"\"\"Average interval width (lower is 'sharper').\"\"\"\n",
    "    return float(np.mean(np.maximum(0.0, q_hi - q_lo)))\n",
    "\n",
    "def scale_normalizer(y):\n",
    "    \"\"\"Robust scale for normalized sharpness.\"\"\"\n",
    "    return np.subtract(*np.percentile(y, [90, 10]))  # P90 - P10\n",
    "\n",
    "print(\"Rows in evaluation:\", len(eval_df))\n",
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ff422",
   "metadata": {},
   "source": [
    "#### Pinball loss (tabular summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinball loss summary across taus\n",
    "rows = []\n",
    "for tau in sorted(models.keys()):\n",
    "    yhat = eval_df[f\"q{int(tau*100)}\"].to_numpy()\n",
    "    rows.append({\n",
    "        \"tau\": tau,\n",
    "        \"pinball_loss\": round(mean_pinball_loss(y_true, yhat, alpha=tau), 4),\n",
    "        \"MAE\": round(np.mean(np.abs(y_true - yhat)), 3)\n",
    "    })\n",
    "pinball_table = pd.DataFrame(rows).sort_values(\"tau\").reset_index(drop=True)\n",
    "pinball_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41055686",
   "metadata": {},
   "source": [
    "#### Empirical coverage (one‑sided for each tau)\n",
    "- Target is ≈τ. Large deviations imply miscalibration in certain regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adf6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical coverage: P(y <= q_tau(x)) should ≈ tau\n",
    "cov_rows = []\n",
    "for tau in sorted(models.keys()):\n",
    "    qcol = f\"q{int(tau*100)}\"\n",
    "    cov = empirical_coverage(eval_df[\"y_true\"], eval_df[qcol])\n",
    "    cov_rows.append({\"tau\": tau, \"empirical_coverage\": round(cov, 3)})\n",
    "cov_df = pd.DataFrame(cov_rows)\n",
    "display(cov_df)\n",
    "\n",
    "# Quick bar chart\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([str(r[\"tau\"]) for r in cov_rows], [r[\"empirical_coverage\"] for r in cov_rows])\n",
    "for r in cov_rows:\n",
    "    plt.axhline(r[\"tau\"], linestyle=\"--\", linewidth=1)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Empirical coverage  (P[y ≤ q̂τ])\")\n",
    "plt.xlabel(\"τ\")\n",
    "plt.title(\"Calibration — Empirical coverage vs target τ\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82263d",
   "metadata": {},
   "source": [
    "#### Interval sharpness (requires both median & P90; normalizes too)\n",
    "- Smaller average width is better (trade‑off with coverage); a normalized version lets you compare across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval sharpness between median (P50) and P90\n",
    "needs = {\"q50\", \"q90\"}\n",
    "if needs.issubset(set(eval_df.columns)):\n",
    "    width = eval_df[\"q90\"] - eval_df[\"q50\"]\n",
    "    avg_width = sharpness_width(eval_df[\"q90\"], eval_df[\"q50\"])\n",
    "    norm = scale_normalizer(eval_df[\"y_true\"])\n",
    "    norm_width = avg_width / (norm if norm > 0 else 1.0)\n",
    "\n",
    "    print(f\"Average width (P50→P90): {avg_width:.3f}\")\n",
    "    print(f\"Normalized width (÷(P90−P10) of y): {norm_width:.3f}\")\n",
    "\n",
    "    # Plot width vs a key driver (choose one you care about)\n",
    "    key = \"search_spend\" if \"search_spend\" in eval_df.columns else X_cols[0]\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.scatter(eval_df[key], width, alpha=0.35, s=14)\n",
    "    plt.xlabel(key); plt.ylabel(\"Interval width (q90 − q50)\")\n",
    "    plt.title(\"Interval sharpness across feature values\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"Sharpness: need both q50 and q90 predictions; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf7d34",
   "metadata": {},
   "source": [
    "#### Calibration curves (bin by predicted quantile)\n",
    "- Show how often actuals fall below predicted quantiles across the range of predictions (reference = horizontal line at τ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves per τ: bin by predicted q̂τ and check empirical P(y ≤ q̂τ)\n",
    "def calibration_curve(y, qhat, n_bins=10):\n",
    "    order = np.argsort(qhat)\n",
    "    y_sorted = y[order]\n",
    "    q_sorted = qhat[order]\n",
    "    bins = np.array_split(np.arange(len(y_sorted)), n_bins)\n",
    "    frac = []\n",
    "    q_mid = []\n",
    "    for b in bins:\n",
    "        frac.append(np.mean(y_sorted[b] <= q_sorted[b]))\n",
    "        q_mid.append(np.median(q_sorted[b]))\n",
    "    return np.array(q_mid), np.array(frac)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "for tau in sorted(models.keys()):\n",
    "    q = eval_df[f\"q{int(tau*100)}\"].to_numpy()\n",
    "    q_mid, frac = calibration_curve(eval_df[\"y_true\"].to_numpy(), q, n_bins=10)\n",
    "    plt.plot(q_mid, frac, marker=\"o\", label=f\"τ={tau:.2f}\")\n",
    "# Ideal reference lines: horizontal at τ (not a straight diagonal!)\n",
    "for tau in sorted(models.keys()):\n",
    "    plt.axhline(tau, linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(\"Predicted quantile value q̂τ\")\n",
    "plt.ylabel(\"Empirical P(y ≤ q̂τ)\")\n",
    "plt.title(\"Calibration curves by predicted quantile value\")\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd544e4d",
   "metadata": {},
   "source": [
    "#### Rolling validation (time‑aware pinball, coverage, sharpness)\n",
    "- See stability/drift over time for pinball, coverage, and interval width. Adjust window to your cadence (e.g., 12 weeks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling validation over time (if a date column exists)\n",
    "date_col = \"date\" if \"date\" in eval_df.columns else None\n",
    "if date_col is None:\n",
    "    # Try to infer a datetime index\n",
    "    if np.issubdtype(eval_df.index.dtype, np.datetime64):\n",
    "        eval_df = eval_df.reset_index().rename(columns={\"index\": \"date\"})\n",
    "        date_col = \"date\"\n",
    "\n",
    "if date_col is None:\n",
    "    print(\"No 'date' column or datetime index found; skipping rolling validation.\")\n",
    "else:\n",
    "    # Ensure datetime & sort\n",
    "    eval_df[date_col] = pd.to_datetime(eval_df[date_col], errors=\"coerce\")\n",
    "    ev = eval_df.dropna(subset=[date_col]).sort_values(date_col).copy()\n",
    "\n",
    "    window = 56  # ~8 weeks if daily; adjust to your cadence\n",
    "    metrics = []\n",
    "    for start in range(0, len(ev) - window + 1):\n",
    "        sub = ev.iloc[start:start+window]\n",
    "        row = {\n",
    "            \"end_date\": sub[date_col].iloc[-1],\n",
    "        }\n",
    "        # Pinball & coverage for each tau\n",
    "        for tau in sorted(models.keys()):\n",
    "            qcol = f\"q{int(tau*100)}\"\n",
    "            row[f\"pinball_tau{int(tau*100)}\"] = mean_pinball_loss(sub[\"y_true\"], sub[qcol], alpha=tau)\n",
    "            row[f\"cover_tau{int(tau*100)}\"]   = empirical_coverage(sub[\"y_true\"], sub[qcol])\n",
    "        # Sharpness (if both available)\n",
    "        if {\"q50\",\"q90\"}.issubset(set(sub.columns)):\n",
    "            row[\"sharp_width_p50_p90\"] = sharpness_width(sub[\"q90\"], sub[\"q50\"])\n",
    "        metrics.append(row)\n",
    "\n",
    "    roll = pd.DataFrame(metrics)\n",
    "\n",
    "    # Plot rolling pinball for τ=0.5 and τ=0.9 (if present)\n",
    "    plt.figure(figsize=(9,4))\n",
    "    if \"pinball_tau50\" in roll.columns:\n",
    "        plt.plot(roll[\"end_date\"], roll[\"pinball_tau50\"], label=\"Pinball τ=0.5\")\n",
    "    if \"pinball_tau90\" in roll.columns:\n",
    "        plt.plot(roll[\"end_date\"], roll[\"pinball_tau90\"], label=\"Pinball τ=0.9\")\n",
    "    plt.title(\"Rolling Pinball Loss\"); plt.xlabel(\"End date\"); plt.ylabel(\"Loss\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Rolling coverage\n",
    "    plt.figure(figsize=(9,4))\n",
    "    if \"cover_tau50\" in roll.columns:\n",
    "        plt.plot(roll[\"end_date\"], roll[\"cover_tau50\"], label=\"Coverage τ=0.5\")\n",
    "        plt.axhline(0.5, linestyle=\"--\", linewidth=1)\n",
    "    if \"cover_tau90\" in roll.columns:\n",
    "        plt.plot(roll[\"end_date\"], roll[\"cover_tau90\"], label=\"Coverage τ=0.9\")\n",
    "        plt.axhline(0.9, linestyle=\"--\", linewidth=1)\n",
    "    plt.title(\"Rolling Empirical Coverage\"); plt.xlabel(\"End date\"); plt.ylabel(\"P(y ≤ q̂τ)\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Rolling sharpness\n",
    "    if \"sharp_width_p50_p90\" in roll.columns:\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(roll[\"end_date\"], roll[\"sharp_width_p50_p90\"], label=\"Width q90−q50\")\n",
    "        plt.title(\"Rolling Interval Sharpness (q90−q50)\"); plt.xlabel(\"End date\"); plt.ylabel(\"Width\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    roll.tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
