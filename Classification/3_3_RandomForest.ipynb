{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce950f11",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Train a RandomForestClassifier and examine OOB score (if enabled) and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Classification/classification_utils.py\n",
    "import classification_utils as utils\n",
    "csv_path = \"https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Classification/classification.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"ts\"]).sort_values(\"ts\")\n",
    "train, test = utils.chrono_split(df, \"ts\", test_frac=0.2)\n",
    "\n",
    "features = [\"ad_channel\",\"device\",\"region\",\"campaign\",\"spend_l7\",\"pages_per_session\",\"sessions_l30\",\"time_on_site_s\",\"pricing_views_l7\",\"email_opens_l30\",\"past_purchases\",\"tenure_days\",\"discount_flag\",\"competitor_visits\"]\n",
    "target = \"converted\"\n",
    "\n",
    "X_train, y_train = train[features], train[target]\n",
    "X_test, y_test = test[features], test[target]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"spend_l7\",\"pages_per_session\",\"sessions_l30\",\"time_on_site_s\",\"pricing_views_l7\",\"email_opens_l30\",\"past_purchases\",\"tenure_days\"]),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"ad_channel\",\"device\",\"region\",\"campaign\"]),\n",
    "    (\"bin\", \"passthrough\", [\"discount_flag\",\"competitor_visits\"])\n",
    "])\n",
    "\n",
    "rf = Pipeline([(\"pre\", pre), (\"clf\", RandomForestClassifier(n_estimators=300, max_depth=None, min_samples_leaf=20, n_jobs=-1, random_state=42, oob_score=False))])\n",
    "rf.fit(X_train, y_train)\n",
    "probs = rf.predict_proba(X_test)[:,1]\n",
    "_ = utils.evaluate_classifier(y_test, probs, title_prefix=\"Random Forest\")\n",
    "\n",
    "# Feature importances\n",
    "est = rf.named_steps[\"clf\"]\n",
    "imp = est.feature_importances_\n",
    "fn = rf.named_steps[\"pre\"].get_feature_names_out()\n",
    "imp_df = pd.DataFrame({\"feature\": fn, \"importance\": imp}).sort_values(\"importance\", ascending=False)\n",
    "imp_df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37a09e",
   "metadata": {},
   "source": [
    "### Advanced diagnostics\n",
    "#### OOB vs Validation PR-AUC / ROC-AUC (side-by-side)\n",
    "- OOB uses the RF’s internal bootstrap to approximate out-of-sample performance on the train set; validation metrics use your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4393dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an OOB-enabled RF with same params\n",
    "rf_oob = clone(rf)\n",
    "rf_oob.set_params(clf__oob_score=True, clf__n_estimators=300)  # ensure enough trees for stable OOB\n",
    "rf_oob.fit(X_train, y_train)\n",
    "\n",
    "# OOB probabilities (class 1) from the RF stage\n",
    "oob_probs = rf_oob.named_steps[\"clf\"].oob_decision_function_[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_probs = probs  # from your fitted rf on test\n",
    "oob_roc = roc_auc_score(y_train, oob_probs)\n",
    "oob_pr  = average_precision_score(y_train, oob_probs)\n",
    "val_roc = roc_auc_score(y_test,  val_probs)\n",
    "val_pr  = average_precision_score(y_test,  val_probs)\n",
    "\n",
    "print(f\"OOB → ROC-AUC={oob_roc:.3f}  PR-AUC={oob_pr:.3f}\")\n",
    "print(f\"VAL → ROC-AUC={val_roc:.3f}  PR-AUC={val_pr:.3f}\")\n",
    "\n",
    "# Simple side-by-side bars\n",
    "labels = [\"ROC-AUC\",\"PR-AUC\"]\n",
    "oob_vals = [oob_roc, oob_pr]\n",
    "val_vals = [val_roc, val_pr]\n",
    "\n",
    "x = np.arange(len(labels)); w = 0.35\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(x - w/2, oob_vals, width=w, label=\"OOB\")\n",
    "plt.bar(x + w/2, val_vals, width=w, label=\"Validation\")\n",
    "plt.xticks(x, labels); plt.ylim(0,1); plt.ylabel(\"Score\"); plt.title(\"OOB vs Validation AUCs\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd93a0",
   "metadata": {},
   "source": [
    "#### Confusion matrix + threshold/utility curve (proposed operating point)\n",
    "- Utility/threshold uses a simple expected-value formula; set gains/costs to your economics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economics (customize)\n",
    "gain_tp = 100.0   # benefit when acting on a true positive\n",
    "cost_fp = 10.0    # cost when acting on a false positive\n",
    "cost_fn = 40.0    # opportunity cost of missing a positive\n",
    "cost_tn = 0.0\n",
    "\n",
    "def expected_value(y_true, p, t, g_tp=gain_tp, c_fp=cost_fp, c_fn=cost_fn, c_tn=cost_tn):\n",
    "    yhat = (p >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, yhat).ravel()\n",
    "    return tp*g_tp - fp*c_fp - fn*c_fn - tn*c_tn\n",
    "\n",
    "grid = np.linspace(0.01, 0.99, 99)\n",
    "evs = [expected_value(y_test, probs, t) for t in grid]\n",
    "t_star = float(grid[int(np.argmax(evs))])\n",
    "print(f\"Proposed threshold (EV-opt): {t_star:.2f}\")\n",
    "\n",
    "# Confusion matrix at t_star\n",
    "yhat = (probs >= t_star).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, yhat).ravel()\n",
    "print(f\"TP={tp} FP={fp} FN={fn} TN={tn}\")\n",
    "\n",
    "# Plot utility curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(grid, evs)\n",
    "plt.axvline(t_star, linestyle=\"--\")\n",
    "plt.xlabel(\"threshold\"); plt.ylabel(\"Expected value\")\n",
    "plt.title(\"Threshold vs Expected Value (validation)\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2d893",
   "metadata": {},
   "source": [
    "#### Reliability (calibration) curves pre/post calibration (+ Brier)\n",
    "- Calibration uses isotonic (non-parametric). Compare Brier scores and reliability curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cal = CalibratedClassifierCV(estimator=clone(rf), method=\"isotonic\", cv=3)\n",
    "rf_cal.fit(X_train, y_train)\n",
    "probs_cal = rf_cal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "brier_raw = brier_score_loss(y_test, probs)\n",
    "brier_cal = brier_score_loss(y_test, probs_cal)\n",
    "print(f\"Brier (raw): {brier_raw:.4f}  |  Brier (calibrated): {brier_cal:.4f}\")\n",
    "\n",
    "pt_raw, pp_raw = calibration_curve(y_test, probs,     n_bins=10, strategy=\"quantile\")\n",
    "pt_cal, pp_cal = calibration_curve(y_test, probs_cal, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(pp_raw, pt_raw, marker=\"o\", label=\"raw\")\n",
    "plt.plot(pp_cal, pt_cal, marker=\"o\", label=\"calibrated\")\n",
    "plt.plot([0,1],[0,1],'--',lw=1,label=\"perfect\")\n",
    "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Observed frequency\")\n",
    "plt.title(\"Calibration (validation)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1383f53",
   "metadata": {},
   "source": [
    "#### Fairness metrics by segment at the operating threshold\n",
    "\n",
    "- Fairness metrics give TPR/FPR/Precision/SelectionRate per segment, plus disparity ranges; align with your org’s fairness policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ee5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_report(df_eval, seg_col, y_true, y_prob, t):\n",
    "    rows = []\n",
    "    for val, idx in df_eval.groupby(seg_col).groups.items():\n",
    "        yt = y_true.loc[idx]\n",
    "        yp = y_prob.loc[idx]\n",
    "        yhat = (yp >= t).astype(int)\n",
    "        tp = int(((yhat==1) & (yt==1)).sum())\n",
    "        fp = int(((yhat==1) & (yt==0)).sum())\n",
    "        fn = int(((yhat==0) & (yt==1)).sum())\n",
    "        tn = int(((yhat==0) & (yt==0)).sum())\n",
    "        tpr = tp / (tp + fn + 1e-12)\n",
    "        fpr = fp / (fp + tn + 1e-12)\n",
    "        prec = tp / (tp + fp + 1e-12)\n",
    "        sel = (yhat==1).mean()\n",
    "        rows.append({\"segment\": seg_col, \"value\": val, \"n\": len(idx),\n",
    "                     \"TPR\": tpr, \"FPR\": fpr, \"Precision\": prec, \"SelectionRate\": sel})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "test_eval = test.reset_index(drop=True)\n",
    "y_true_s = pd.Series(y_test.values, index=test_eval.index, name=\"y_true\")\n",
    "y_prob_s = pd.Series(probs,       index=test_eval.index, name=\"y_prob\")\n",
    "\n",
    "segments = [c for c in [\"device\",\"region\",\"ad_channel\",\"campaign\"] if c in test_eval.columns]\n",
    "fair_list = [seg_report(test_eval, seg, y_true_s, y_prob_s, t_star) for seg in segments]\n",
    "fair_df = pd.concat(fair_list, ignore_index=True)\n",
    "display(fair_df.sort_values([\"segment\",\"value\"]))\n",
    "\n",
    "# Disparity summary\n",
    "disp = (fair_df.groupby(\"segment\")[[\"TPR\",\"FPR\",\"SelectionRate\",\"Precision\"]]\n",
    "               .agg(lambda s: float(s.max() - s.min())).reset_index())\n",
    "disp.columns = [\"segment\",\"ΔTPR\",\"ΔFPR\",\"ΔSelection\",\"ΔPrecision\"]\n",
    "display(disp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d0d84",
   "metadata": {},
   "source": [
    "#### Plain-language summary(example): aggregate importances to original features & translate to policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97156201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformed features back to original columns\n",
    "fn_all = rf.named_steps[\"pre\"].get_feature_names_out()\n",
    "imp_all = rf.named_steps[\"clf\"].feature_importances_\n",
    "\n",
    "def to_original(feat_name: str) -> str:\n",
    "    # Strip transformer prefixes if present\n",
    "    if \"__\" in feat_name:\n",
    "        feat_name = feat_name.split(\"__\",1)[1]\n",
    "    # Map OHE groups to their base feature\n",
    "    for base in [\"ad_channel\",\"device\",\"region\",\"campaign\"]:\n",
    "        if feat_name.startswith(base + \"_\"):\n",
    "            return base\n",
    "    # Numeric or binary passthroughs\n",
    "    for base in [\"spend_l7\",\"pages_per_session\",\"sessions_l30\",\"time_on_site_s\",\n",
    "                 \"pricing_views_l7\",\"email_opens_l30\",\"past_purchases\",\"tenure_days\",\n",
    "                 \"discount_flag\",\"competitor_visits\"]:\n",
    "        if feat_name.startswith(base):\n",
    "            return base\n",
    "    return feat_name\n",
    "\n",
    "orig_map = pd.Series([to_original(f) for f in fn_all], index=fn_all)\n",
    "agg = (pd.DataFrame({\"orig_feature\": orig_map, \"importance\": imp_all})\n",
    "         .groupby(\"orig_feature\", as_index=False)[\"importance\"].sum()\n",
    "         .sort_values(\"importance\", ascending=False)\n",
    "         .reset_index(drop=True))\n",
    "agg[\"cum_share\"] = agg[\"importance\"].cumsum() / agg[\"importance\"].sum()\n",
    "\n",
    "display(agg)\n",
    "\n",
    "# Plain-language summary to ~70% coverage\n",
    "top = agg[agg[\"cum_share\"] <= 0.70]\n",
    "if top.empty:\n",
    "    top = agg.head(1)\n",
    "covered = top[\"importance\"].sum() / agg[\"importance\"].sum()\n",
    "feat_list = \", \".join(top[\"orig_feature\"].tolist())\n",
    "print(f\"Policy summary: The model’s decisions are driven mainly by {feat_list}, \"\n",
    "      f\"which together account for ~{covered*100:.0f}% of feature importance. \"\n",
    "      \"Prioritize data quality and governance for these fields, and consider segment-specific policies \"\n",
    "      \"if fairness gaps align with them.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
