{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cade96",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Fit a DecisionTreeClassifier, tune depth minimally, and inspect feature importance and decision boundaries via partial plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Classification/classification_utils.py\n",
    "import classification_utils as utils\n",
    "csv_path = \"https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Classification/classification.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"ts\"]).sort_values(\"ts\")\n",
    "train, test = utils.chrono_split(df, \"ts\", test_frac=0.2)\n",
    "\n",
    "features = [\"ad_channel\",\"device\",\"region\",\"campaign\",\"spend_l7\",\"pages_per_session\",\"sessions_l30\",\"time_on_site_s\",\"pricing_views_l7\",\"past_purchases\",\"discount_flag\",\"competitor_visits\"]\n",
    "target = \"converted\"\n",
    "\n",
    "X_train, y_train = train[features], train[target]\n",
    "X_test, y_test = test[features], test[target]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"spend_l7\",\"pages_per_session\",\"sessions_l30\",\"time_on_site_s\",\"pricing_views_l7\",\"past_purchases\"]),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"ad_channel\",\"device\",\"region\",\"campaign\"]),\n",
    "    (\"bin\", \"passthrough\", [\"discount_flag\",\"competitor_visits\"])\n",
    "])\n",
    "\n",
    "tree = Pipeline([(\"pre\", pre), (\"clf\", DecisionTreeClassifier(max_depth=6, min_samples_leaf=50, random_state=42))])\n",
    "tree.fit(X_train, y_train)\n",
    "probs = tree.predict_proba(X_test)[:,1]\n",
    "_ = utils.evaluate_classifier(y_test, probs, title_prefix=\"Decision Tree\")\n",
    "\n",
    "# Show a small tree plot\n",
    "est = tree.named_steps[\"clf\"]\n",
    "plt.figure(figsize=(12,6)); plot_tree(est, fontsize=10, max_depth=2, filled=True, feature_names=tree.named_steps[\"pre\"].get_feature_names_out(), class_names=[\"no\",\"yes\"]); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf46c2a",
   "metadata": {},
   "source": [
    "### Advanced Dignostics\n",
    "#### The top of the tree (depth ≤ 3) with sample counts & class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c329be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text view of the top of the tree (depth ≤ 3) with counts & probabilities\n",
    "import numpy as np\n",
    "from sklearn.tree import _tree\n",
    "\n",
    "est = tree.named_steps[\"clf\"]\n",
    "fn = tree.named_steps[\"pre\"].get_feature_names_out()\n",
    "\n",
    "T = est.tree_\n",
    "n_classes = T.value.shape[2]\n",
    "assert n_classes == 2, \"This snippet assumes a binary target.\"\n",
    "\n",
    "def node_summary(node_id):\n",
    "    counts = T.value[node_id, 0]  # [neg, pos]\n",
    "    total = counts.sum()\n",
    "    p_pos = counts[1] / total if total > 0 else np.nan\n",
    "    return total, counts[1], p_pos\n",
    "\n",
    "def print_subtree(node_id=0, depth=0, max_depth=3, prefix=\"\"):\n",
    "    total, pos, p_pos = node_summary(node_id)\n",
    "    indent = \"  \" * depth\n",
    "    if T.feature[node_id] != _tree.TREE_UNDEFINED and depth < max_depth:\n",
    "        feat = fn[T.feature[node_id]]\n",
    "        thresh = T.threshold[node_id]\n",
    "        print(f\"{indent}{prefix}Node {node_id}: IF {feat} ≤ {thresh:.3f} \"\n",
    "              f\"(n={int(total)}, pos={int(pos)}, p1={p_pos:.3f})\")\n",
    "        print_subtree(T.children_left[node_id], depth+1, max_depth, prefix=\"Left:  \")\n",
    "        print(f\"{indent}{prefix}ELSE {feat} > {thresh:.3f} \"\n",
    "              f\"(n={int(total)}, pos={int(pos)}, p1={p_pos:.3f})\")\n",
    "        print_subtree(T.children_right[node_id], depth+1, max_depth, prefix=\"Right: \")\n",
    "    else:\n",
    "        print(f\"{indent}{prefix}Leaf {node_id}: (n={int(total)}, pos={int(pos)}, p1={p_pos:.3f})\")\n",
    "\n",
    "print_subtree(node_id=0, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f36ae7",
   "metadata": {},
   "source": [
    "#### PR‑AUC / ROC‑AUC + confusion matrix at a proposed threshold (F1‑optimal)\n",
    "- Picks F1‑optimal threshold on test for illustration. In production, pick thresholds on a validation set and lock them before final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97978ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR‑AUC / ROC‑AUC and confusion matrix at proposed threshold (F1-opt)\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, roc_curve,\n",
    "                             precision_recall_curve, confusion_matrix, f1_score)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, probs)\n",
    "pr_auc  = average_precision_score(y_test, probs)\n",
    "print(f\"ROC‑AUC: {roc_auc:.3f}  |  PR‑AUC: {pr_auc:.3f}\")\n",
    "\n",
    "# Curves\n",
    "fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "prec, rec, _ = precision_recall_curve(y_test, probs)\n",
    "\n",
    "plt.figure(figsize=(5,4)); plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--',lw=1); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC\"); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4)); plt.plot(rec, prec, label=f\"AP={pr_auc:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision‑Recall\"); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Proposed threshold: maximize F1 on test (swap to validation if you prefer)\n",
    "grid = np.linspace(0.01, 0.99, 99)\n",
    "f1s = [f1_score(y_test, (probs >= t).astype(int)) for t in grid]\n",
    "t_star = float(grid[int(np.argmax(f1s))])\n",
    "print(f\"Proposed threshold (F1‑optimal): {t_star:.2f}\")\n",
    "\n",
    "# Confusion matrix at t_star\n",
    "yhat = (probs >= t_star).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, yhat).ravel()\n",
    "print(f\"TP={tp} FP={fp} FN={fn} TN={tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea899c99",
   "metadata": {},
   "source": [
    "#### Calibration curves pre vs post calibration (+ Brier score)\n",
    "- Calibrates with isotonic via CalibratedClassifierCV (3‑fold on train), then compares calibration curves and Brier scores.\n",
    "\n",
    "- The Brier score measures how accurate a model's predicted probabilities are to the actual outcomes, providing a single number for probabilistic accuracy, while ROC AUC measures a model's ability to distinguish between classes, essentially ranking predictions. A lower Brier score indicates better accuracy, with 0 being perfect, whereas a higher ROC AUC (closer to 1) indicates better discrimination between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af09626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves pre/post calibration (isotonic) and Brier scores\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Fit a calibrated version of the SAME pipeline (fresh clone)\n",
    "base = clone(tree)\n",
    "cal = CalibratedClassifierCV(estimator=base, method=\"isotonic\", cv=3)\n",
    "cal.fit(X_train, y_train)\n",
    "probs_cal = cal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Brier scores\n",
    "brier_raw = brier_score_loss(y_test, probs)\n",
    "brier_cal = brier_score_loss(y_test, probs_cal)\n",
    "print(f\"Brier (raw): {brier_raw:.4f}  |  Brier (calibrated): {brier_cal:.4f}\")\n",
    "\n",
    "# Reliability curves\n",
    "pt_raw, pp_raw = calibration_curve(y_test, probs, n_bins=10, strategy=\"quantile\")\n",
    "pt_cal, pp_cal = calibration_curve(y_test, probs_cal, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(pp_raw, pt_raw, marker=\"o\", label=\"raw\")\n",
    "plt.plot(pp_cal, pt_cal, marker=\"o\", label=\"calibrated\")\n",
    "plt.plot([0,1],[0,1],'--',lw=1, label=\"perfect\")\n",
    "plt.xlabel(\"Predicted probability (bin avg)\")\n",
    "plt.ylabel(\"Observed frequency\")\n",
    "plt.title(\"Calibration (test)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7e37c",
   "metadata": {},
   "source": [
    "#### Stability of top splits across folds/quarters\n",
    "- Uses quarters from the ts column on train to test split‑stability of the root split feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c63130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability of top splits across quarters (time-aware)\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "def root_feature_name(pipe, X, y):\n",
    "    model = clone(pipe)\n",
    "    model.fit(X, y)\n",
    "    est = model.named_steps[\"clf\"]\n",
    "    pre = model.named_steps[\"pre\"]\n",
    "    feats = pre.get_feature_names_out()\n",
    "    fi = est.tree_.feature[0]\n",
    "    return feats[fi] if fi >= 0 else \"LEAF\"\n",
    "\n",
    "# Build quarter labels on TRAIN only\n",
    "train_q = train.copy()\n",
    "train_q[\"quarter\"] = train_q[\"ts\"].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "root_by_q = []\n",
    "for q, dfq in train_q.groupby(\"quarter\"):\n",
    "    if dfq[target].nunique() < 2:\n",
    "        continue\n",
    "    Xq, yq = dfq[features], dfq[target]\n",
    "    root = root_feature_name(tree, Xq, yq)\n",
    "    root_by_q.append({\"quarter\": q, \"root_split\": root})\n",
    "\n",
    "stab_df = pd.DataFrame(root_by_q)\n",
    "display(stab_df)\n",
    "\n",
    "# Frequency of root features\n",
    "freq = stab_df[\"root_split\"].value_counts().reset_index()\n",
    "freq.columns = [\"feature\", \"count\"]\n",
    "display(freq)\n",
    "\n",
    "# Quick bar plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(freq[\"feature\"], freq[\"count\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"#quarters as root split\")\n",
    "plt.title(\"Stability of root split across quarters (train)\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9aaf78",
   "metadata": {},
   "source": [
    "#### Fairness check across predefined segments (e.g., device / region) at t_star\n",
    "- Reports equal opportunity (TPR), FPR, precision, and selection rate per segment + disparity ranges; adjust segments to your policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c70ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness metrics across segments at threshold t_star\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def seg_report(df_eval, seg_col, y_true, y_prob, t):\n",
    "    out = []\n",
    "    for val, idx in df_eval.groupby(seg_col).groups.items():\n",
    "        yt = y_true.loc[idx]\n",
    "        yp = y_prob.loc[idx]\n",
    "        yhat = (yp >= t).astype(int)\n",
    "        tp = int(((yhat==1) & (yt==1)).sum())\n",
    "        fp = int(((yhat==1) & (yt==0)).sum())\n",
    "        fn = int(((yhat==0) & (yt==1)).sum())\n",
    "        tn = int(((yhat==0) & (yt==0)).sum())\n",
    "        tpr = tp / (tp + fn + 1e-12)  # recall / equal opportunity\n",
    "        fpr = fp / (fp + tn + 1e-12)\n",
    "        prec = tp / (tp + fp + 1e-12)\n",
    "        sel = (yhat==1).mean()        # selection rate / demographic parity\n",
    "        out.append({\"segment\": seg_col, \"value\": val, \"n\": len(idx),\n",
    "                    \"TPR\": tpr, \"FPR\": fpr, \"Precision\": prec, \"SelectionRate\": sel})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Build aligned frames for test split\n",
    "test_eval = test.reset_index(drop=False).rename(columns={\"index\":\"_idx\"})\n",
    "y_true_s = pd.Series(y_test.values, index=test_eval.index, name=\"y_true\")\n",
    "y_prob_s = pd.Series(probs, index=test_eval.index, name=\"y_prob\")\n",
    "\n",
    "segments = [c for c in [\"device\",\"region\",\"ad_channel\",\"campaign\"] if c in test_eval.columns]\n",
    "\n",
    "fair_tables = []\n",
    "for seg in segments:\n",
    "    fair_tables.append(seg_report(test_eval, seg, y_true_s, y_prob_s, t_star))\n",
    "fair_df = pd.concat(fair_tables, ignore_index=True)\n",
    "display(fair_df.sort_values([\"segment\",\"value\"]))\n",
    "\n",
    "# Disparity summary (max ‑ min across groups per metric)\n",
    "disp = (fair_df.groupby(\"segment\")[[\"TPR\",\"FPR\",\"SelectionRate\",\"Precision\"]]\n",
    "               .agg(lambda s: s.max() - s.min()).reset_index())\n",
    "disp.columns = [\"segment\",\"ΔTPR\",\"ΔFPR\",\"ΔSelection\",\"ΔPrecision\"]\n",
    "display(disp)\n",
    "\n",
    "# Optional: visualize SelectionRate disparity per segment\n",
    "for seg in segments:\n",
    "    sub = fair_df[fair_df[\"segment\"]==seg]\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.bar(sub[\"value\"].astype(str), sub[\"SelectionRate\"])\n",
    "    plt.title(f\"Selection rate by {seg} (t={t_star:.2f})\"); plt.ylim(0,1)\n",
    "    plt.xticks(rotation=30, ha=\"right\"); plt.ylabel(\"Selection rate\")\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3759331",
   "metadata": {},
   "source": [
    "### (Advanced) Rules for Interpreting Segment Diagnostics Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845669f",
   "metadata": {},
   "source": [
    "1. Recall (TPR) across groups\n",
    "    - Rule: High TPR = the model is catching most positives; low TPR = missing many true converters.\n",
    "    - How to use: Compare TPR across segments; a gap >5–10% signals disparity in model sensitivity.\n",
    "\n",
    "2. False Positive Rate (FPR) differences\n",
    "    - Rule: High FPR = model incorrectly flags too many negatives as positives.\n",
    "    - How to use: If one group has consistently higher FPR, that segment gets “over-scored” → wasted spend.\n",
    "\n",
    "3. Balance TPR vs FPR\n",
    "    - Rule: High recall + high FPR means “catch everything but wasteful.”\n",
    "    - Rule: Low recall + low FPR means “conservative but misses opportunities.”\n",
    "    - How to use: Identify which strategy each segment is implicitly following.\n",
    "\n",
    "4. Interpret Precision as ROI efficiency\n",
    "    - Rule: Precision = of those we target, how many truly convert.\n",
    "    - How to use: Higher precision → better ROI; lower precision → wasted actions.\n",
    "    - Tip: Compare precision across segments to see where marketing dollars go furthest.\n",
    "\n",
    "5. Use Selection Rate to check bias/exposure\n",
    "    - Rule: Selection Rate = % of group predicted positive.\n",
    "    - How to use: Very different selection rates between groups → model treats groups unequally, could reflect bias.\n",
    "\n",
    "6. Consistency across metrics\n",
    "    - Rule: Don’t just pick one metric; e.g., high TPR but low precision tells a different story than balanced performance.\n",
    "    - How to use: Triangulate:\n",
    "    - High TPR + High FPR = overpredictive\n",
    "    - High Precision + Low TPR = underpredictive\n",
    "    - Balanced = efficient\n",
    "\n",
    "7. Disparity ranges (ΔTPR, ΔFPR, etc.)\n",
    "    - Rule: Disparity = max – min across groups.\n",
    "    - How to use: Large disparities mean fairness/consistency concerns. Small disparities → stable across groups.\n",
    "\n",
    "8. Tie metrics back to business\n",
    "    - Rule: Precision links to ROI efficiency, Recall links to market coverage, FPR links to wasted spend / risk.\n",
    "    - How to use: Interpret differences not just as stats, but as “where we overspend, underserve, or misallocate.”\n",
    "\n",
    "9. Volume matters (the n column)\n",
    "    - Rule: Large groups dominate business impact even if gaps are small. Small groups may be noisy but less critical.\n",
    "    - How to use: Always weigh metrics against group size before prioritizing fixes.\n",
    "\n",
    "10. Systematic patterns\n",
    "    - Rule: If multiple segments (e.g., certain channels or regions) show the same skew (e.g., lower precision), that’s a structural model bias.\n",
    "    - How to use: Flag as “model design issue” vs. “random noise.”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
