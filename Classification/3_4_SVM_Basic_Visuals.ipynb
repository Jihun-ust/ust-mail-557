{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adefacf1",
   "metadata": {},
   "source": [
    "\n",
    "# SVM — How Maximizing the Margin Works (Visual Guide)\n",
    "\n",
    "This notebook shows, step by step, how a **Support Vector Machine (SVM)** finds a boundary that **maximizes the margin** between classes.\n",
    "\n",
    "**Features:**\n",
    "1. A linear SVM on simple 2D data with the separating hyperplane, **margin lines**, and **support vectors**.\n",
    "2. How to compute the **margin width** \\(2/\\|w\\|\\) and visualize distances via the decision function.\n",
    "3. The effect of **C** (regularization) on the margin: small C → wider margin & more violations; large C → narrower margin & fewer violations.\n",
    "4. **Hinge loss** intuition: why misclassified or within-margin points are penalized.\n",
    "5. A **nonseparable** dataset to contrast small vs large C behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b832003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b7e9d",
   "metadata": {},
   "source": [
    "\n",
    "## Linear SVM on a simple 2D dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47adb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate separable data\n",
    "n = 200\n",
    "X_pos = np.random.randn(n//2, 2) * 0.6 + np.array([2.5, 2.5])\n",
    "X_neg = np.random.randn(n//2, 2) * 0.6 + np.array([-2.5, -2.0])\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "y = np.hstack([np.ones(n//2), -np.ones(n//2)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=1.0, random_state=0))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test accuracy:\", round(accuracy_score(y_test, clf.predict(X_test)), 3))\n",
    "\n",
    "svm = clf.named_steps[\"svc\"]\n",
    "\n",
    "x_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0\n",
    "y_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n",
    "                     np.linspace(y_min, y_max, 400))\n",
    "XY = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = clf.decision_function(XY).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.contourf(xx, yy, Z>0, alpha=0.15)\n",
    "plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\",\"-\",\"--\"])\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=(y_train>0).astype(int), s=20, alpha=0.8, label=\"train\")\n",
    "plt.scatter(X_test[:,0],  X_test[:,1],  c=(y_test>0).astype(int),  s=20, alpha=0.6, marker=\"x\", label=\"test\")\n",
    "\n",
    "sv = svm.support_vectors_\n",
    "scaler = clf.named_steps[\"standardscaler\"]\n",
    "sv_orig = scaler.inverse_transform(sv)\n",
    "plt.scatter(sv_orig[:,0], sv_orig[:,1], s=100, facecolors=\"none\", edgecolors=\"k\", label=\"support vectors\")\n",
    "\n",
    "plt.title(\"Linear SVM: separating hyperplane and margins (±1)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b37ec",
   "metadata": {},
   "source": [
    "\n",
    "## Margin width \\(2/\\|w\\|\\) and distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d793236",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = svm.coef_.ravel()\n",
    "norm_w = np.linalg.norm(w)\n",
    "margin = 2.0 / norm_w\n",
    "print(\"||w|| =\", round(norm_w, 4))\n",
    "print(\"Margin width (2/||w||) =\", round(margin, 4))\n",
    "\n",
    "df_train = clf.decision_function(X_train)\n",
    "df_test  = clf.decision_function(X_test)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df_train, bins=40, alpha=0.7, label=\"train\")\n",
    "plt.hist(df_test,  bins=40, alpha=0.7, label=\"test\")\n",
    "plt.axvline(0, linestyle=\"--\")\n",
    "plt.title(\"Decision function distribution\")\n",
    "plt.xlabel(\"decision function\"); plt.ylabel(\"count\"); plt.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b242e38",
   "metadata": {},
   "source": [
    "\n",
    "## Effect of **C** on margin, support vectors, and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4417d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.05, 0.2, 1, 5, 20]\n",
    "margin_widths = []\n",
    "n_support = []\n",
    "accs = []\n",
    "\n",
    "for C in Cs:\n",
    "    model = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=C, random_state=0))\n",
    "    model.fit(X_train, y_train)\n",
    "    svm_i = model.named_steps[\"svc\"]\n",
    "    w_i = svm_i.coef_.ravel()\n",
    "    m_i = 2.0 / (np.linalg.norm(w_i) + 1e-12)\n",
    "    margin_widths.append(m_i)\n",
    "    n_support.append(int(svm_i.n_support_.sum()))\n",
    "    accs.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(Cs, margin_widths, marker=\"o\")\n",
    "plt.xscale(\"log\"); plt.xlabel(\"C (log scale)\"); plt.ylabel(\"Margin width (2/||w||)\")\n",
    "plt.title(\"Margin width vs C\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(Cs, n_support, marker=\"o\")\n",
    "plt.xscale(\"log\"); plt.xlabel(\"C (log scale)\"); plt.ylabel(\"# support vectors\")\n",
    "plt.title(\"Support vectors vs C\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(Cs, accs, marker=\"o\")\n",
    "plt.xscale(\"log\"); plt.xlabel(\"C (log scale)\"); plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Accuracy vs C\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a499f6",
   "metadata": {},
   "source": [
    "\n",
    "## Hinge loss intuition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-2.5, 2.5, 400)\n",
    "hinge = np.maximum(0.0, 1.0 - z)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(z, hinge)\n",
    "plt.axvline(1.0, linestyle=\"--\")\n",
    "plt.title(\"Hinge loss  ℓ(z) = max(0, 1 - z)\")\n",
    "plt.xlabel(\"z = y · f(x)\"); plt.ylabel(\"loss\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c79b9",
   "metadata": {},
   "source": [
    "\n",
    "## Nonseparable data: small C vs large C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = 300\n",
    "A = np.random.randn(n2//2, 2) * 1.0 + np.array([1.0, 1.0])\n",
    "B = np.random.randn(n2//2, 2) * 1.0 + np.array([-0.5, -0.5])\n",
    "X2 = np.vstack([A, B]); y2 = np.hstack([np.ones(n2//2), -np.ones(n2//2)])\n",
    "\n",
    "X2_tr, X2_te, y2_tr, y2_te = train_test_split(X2, y2, test_size=0.3, random_state=0, stratify=y2)\n",
    "\n",
    "def plot_boundary(model, X, y, title):\n",
    "    x_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0\n",
    "    y_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n",
    "                         np.linspace(y_min, y_max, 400))\n",
    "    XY = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.decision_function(XY).reshape(xx.shape)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.contourf(xx, yy, Z>0, alpha=0.15)\n",
    "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\",\"-\",\"--\"])\n",
    "    plt.scatter(X[:,0], X[:,1], c=(y>0).astype(int), s=18, alpha=0.8)\n",
    "    svm_local = model.named_steps[\"svc\"]\n",
    "    sv_orig = model.named_steps[\"standardscaler\"].inverse_transform(svm_local.support_vectors_)\n",
    "    plt.scatter(sv_orig[:,0], sv_orig[:,1], s=80, facecolors=\"none\", edgecolors=\"k\")\n",
    "    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "model_smallC = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=0.2, random_state=0)).fit(X2_tr, y2_tr)\n",
    "plot_boundary(model_smallC, X2_tr, y2_tr, \"Overlapping data — Small C (wider margin, more violations)\")\n",
    "\n",
    "model_largeC = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=20.0, random_state=0)).fit(X2_tr, y2_tr)\n",
    "plot_boundary(model_largeC, X2_tr, y2_tr, \"Overlapping data — Large C (narrow margin, fewer violations)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
