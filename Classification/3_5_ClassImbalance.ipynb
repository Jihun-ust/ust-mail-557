{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b6313d",
   "metadata": {},
   "source": [
    "# Class Imbalance Techniques (class weights, focal weights, SMOTE)\n",
    "\n",
    "Compare three approaches on the imbalanced **converted** target: class weights in Logistic Regression, focal-like reweighting (iterative), and SMOTE (if `imblearn` is available; else naive oversampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, warnings\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Classification/classification_utils.py\n",
    "import classification_utils as utils\n",
    "csv_path = \"https://raw.githubusercontent.com/Jihun-ust/ust-mail-557/main/Classification/classification.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"ts\"]).sort_values(\"ts\")\n",
    "train, test = utils.chrono_split(df, \"ts\", test_frac=0.2)\n",
    "\n",
    "features = [\"ad_channel\",\"device\",\"region\",\"campaign\",\"spend_l7\",\"pages_per_session\",\"sessions_l30\",\"time_on_site_s\",\"pricing_views_l7\",\"past_purchases\",\"discount_flag\",\"competitor_visits\"]\n",
    "target = \"converted\"\n",
    "X_train, y_train = train[features], train[target]\n",
    "X_test, y_test = test[features], test[target]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [\"spend_l7\",\"pages_per_session\",\"sessions_l30\",\"time_on_site_s\",\"pricing_views_l7\",\"past_purchases\",]),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"ad_channel\",\"device\",\"region\",\"campaign\"]),\n",
    "    (\"bin\", \"passthrough\", [\"discount_flag\",\"competitor_visits\"])\n",
    "])\n",
    "\n",
    "def fit_eval(pipe):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    probs = pipe.predict_proba(X_test)[:,1]\n",
    "    return utils.evaluate_classifier(y_test, probs)\n",
    "\n",
    "# Class weights (balanced)\n",
    "lr_w = Pipeline([(\"pre\", pre), (\"lr\", LogisticRegression(max_iter=600, class_weight=\"balanced\"))])\n",
    "print(\"=== Class weights (balanced) ===\")\n",
    "m1 = fit_eval(lr_w)\n",
    "\n",
    "# Focal-like reweighting (iterative, two rounds)\n",
    "lr = Pipeline([(\"pre\", pre), (\"lr\", LogisticRegression(max_iter=600))])\n",
    "lr.fit(X_train, y_train)\n",
    "p_train = lr.predict_proba(X_train)[:,1]\n",
    "w = utils.focal_reweighting(y_train.values, p_train, gamma=2.0, alpha_pos=0.25)\n",
    "lr_fw = Pipeline([(\"pre\", pre), (\"lr\", LogisticRegression(max_iter=600))])\n",
    "print(\"\\n=== Focal-like reweighting (1 pass) ===\")\n",
    "lr_fw.fit(X_train, y_train, lr__sample_weight=w)\n",
    "p_test = lr_fw.predict_proba(X_test)[:,1]\n",
    "_ = utils.evaluate_classifier(y_test, p_test)\n",
    "\n",
    "# SMOTE (if available) or naive oversampling\n",
    "print(\"\\n=== SMOTE or naive oversampling ===\")\n",
    "try:\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    imb = ImbPipeline([(\"pre\", pre), (\"sm\", SMOTE()), (\"lr\", LogisticRegression(max_iter=600))])\n",
    "    imb.fit(X_train, y_train)\n",
    "    probs = imb.predict_proba(X_test)[:,1]\n",
    "    _ = utils.evaluate_classifier(y_test, probs)\n",
    "    print(\"(Used SMOTE)\")\n",
    "except Exception as e:\n",
    "    Xp = pre.fit_transform(X_train)\n",
    "    Xo, yo = utils.naive_oversample(pd.DataFrame(Xp.toarray() if hasattr(Xp, 'toarray') else Xp), y_train.reset_index(drop=True))\n",
    "    lr_os = LogisticRegression(max_iter=600)\n",
    "    lr_os.fit(Xo, yo)\n",
    "    probs = lr_os.predict_proba(pre.transform(X_test))[:,1]\n",
    "    _ = utils.evaluate_classifier(y_test, probs)\n",
    "    print(\"(Used naive oversampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88491fe",
   "metadata": {},
   "source": [
    "#### Advanced Diagnostic (Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826e0ba",
   "metadata": {},
   "source": [
    "Class Weights (balanced)\n",
    "   - Performance: AUC-ROC ≈ 0.60, PR-AUC ≈ 0.68 → moderate discrimination, better than chance but far from perfect.\n",
    "   - Pattern: Both classes are identified, with recall ~54% for non-converters and ~60% for converters.\n",
    "   - Implication: A fairer balance; the model doesn’t collapse into “always yes” or “always no.” Usable as a baseline.\n",
    "\n",
    "Focal-like Reweighting (1 pass)\n",
    "   - Performance: AUC-ROC 0.40 (worse than random), PR-AUC 0.53 (drop from 0.68).\n",
    "   - Pattern: Model predicts everything as non-converter. Perfect recall on class 0 (100%), but 0% recall on converters.\n",
    "   - Implication: This approach failed; the model ignored converting leads entirely. Not viable for production.\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling)\n",
    "   - Performance: Nearly identical to Class Weights. AUC-ROC ≈ 0.59, PR-AUC ≈ 0.67.\n",
    "   - Pattern: Very similar confusion matrix; precision/recall balance close to Class Weights.\n",
    "   - Implication: Oversampling didn’t improve results beyond simple class weighting. Adds complexity without clear lift.\n",
    "\n",
    "Business Takeaways\n",
    "   - Balanced class weights are the simplest and most reliable option here. They produce moderate but usable balance between catching converters and avoiding false alarms.\n",
    "   - SMOTE doesn’t outperform class weights, no added value unless future data suggests otherwise.\n",
    "   - Focal-like reweighting broke the model, it’s not suitable with current data setup.\n",
    "   - Overall accuracy is still low (~57%). To improve further, leaders should expect:\n",
    "      - Feature engineering (better behavioral signals, campaign interactions).\n",
    "      - Threshold tuning (adjust decision cut-offs to prioritize either precision or recall depending on SLA).\n",
    "      - Alternative models (tree ensembles, calibrated gradient boosting).\n",
    "\n",
    "In plain English: Weighting works, synthetic oversampling doesn’t add much, and focal reweighting failed. Current models are “baseline adequate” but not business-ready; they’ll still misclassify too many leads without better data signals or refined thresholds.\n",
    "\n",
    "**Note**: *The dataset used in this example was programmatically generated, which does not reflect real-world information.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
